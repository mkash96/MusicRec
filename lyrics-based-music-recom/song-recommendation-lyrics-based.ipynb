{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d8a42b",
   "metadata": {
    "papermill": {
     "duration": 0.00745,
     "end_time": "2024-11-23T06:04:25.353011",
     "exception": false,
     "start_time": "2024-11-23T06:04:25.345561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "INSTALL LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4576f904",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:04:25.367625Z",
     "iopub.status.busy": "2024-11-23T06:04:25.367379Z",
     "iopub.status.idle": "2024-11-23T06:04:25.375555Z",
     "shell.execute_reply": "2024-11-23T06:04:25.374728Z"
    },
    "papermill": {
     "duration": 0.017829,
     "end_time": "2024-11-23T06:04:25.377197",
     "exception": false,
     "start_time": "2024-11-23T06:04:25.359368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "cudf-cu11 \n",
    "cuml-cu11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9629ce6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:04:25.390641Z",
     "iopub.status.busy": "2024-11-23T06:04:25.390393Z",
     "iopub.status.idle": "2024-11-23T06:05:20.544022Z",
     "shell.execute_reply": "2024-11-23T06:05:20.543088Z"
    },
    "papermill": {
     "duration": 55.162767,
     "end_time": "2024-11-23T06:05:20.546246",
     "exception": false,
     "start_time": "2024-11-23T06:04:25.383479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\r\n",
      "Collecting cudf-cu11 (from -r requirements.txt (line 1))\r\n",
      "  Downloading https://pypi.nvidia.com/cudf-cu11/cudf_cu11-24.10.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (24.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m307.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cuml-cu11 (from -r requirements.txt (line 2))\r\n",
      "  Downloading https://pypi.nvidia.com/cuml-cu11/cuml_cu11-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (1372.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 GB\u001b[0m \u001b[31m219.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cachetools (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Collecting cubinlinker-cu11 (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading https://pypi.nvidia.com/cubinlinker-cu11/cubinlinker_cu11-0.3.0.post2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m229.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cuda-python<12.0a0,>=11.7.1 (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading cuda_python-11.8.5.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\r\n",
      "Collecting cupy-cuda11x>=12.0.0 (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading cupy_cuda11x-13.3.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.7 kB)\r\n",
      "Collecting fsspec>=0.6.0 (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting libcudf-cu11==24.10.* (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading https://pypi.nvidia.com/libcudf-cu11/libcudf_cu11-24.10.1-py3-none-manylinux_2_28_x86_64.whl (502.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.8/502.8 MB\u001b[0m \u001b[31m238.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting numba>=0.57 (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\r\n",
      "Collecting numpy<3.0a0,>=1.23 (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvtx>=0.2.1 (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading nvtx-0.2.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (740 bytes)\r\n",
      "Collecting packaging (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Collecting pandas<2.2.3dev0,>=2.0 (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\r\n",
      "Collecting ptxcompiler-cu11 (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading https://pypi.nvidia.com/ptxcompiler-cu11/ptxcompiler_cu11-0.8.1.post2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (8.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m312.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pyarrow<18.0.0a0,>=14.0.0 (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\r\n",
      "Collecting pylibcudf-cu11==24.10.* (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading https://pypi.nvidia.com/pylibcudf-cu11/pylibcudf_cu11-24.10.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (20.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m225.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting rich (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\r\n",
      "Collecting rmm-cu11==24.10.* (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading https://pypi.nvidia.com/rmm-cu11/rmm_cu11-24.10.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m278.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting typing_extensions>=4.0.0 (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting cuvs-cu11==24.10.* (from cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading https://pypi.nvidia.com/cuvs-cu11/cuvs_cu11-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (1432.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 GB\u001b[0m \u001b[31m270.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting dask-cuda==24.10.* (from cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading https://pypi.nvidia.com/dask-cuda/dask_cuda-24.10.0-py3-none-any.whl (133 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m219.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting dask-cudf-cu11==24.10.* (from cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading https://pypi.nvidia.com/dask-cudf-cu11/dask_cudf_cu11-24.10.1-py3-none-any.whl (56 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m143.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting joblib>=0.11 (from cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting pylibraft-cu11==24.10.* (from cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading https://pypi.nvidia.com/pylibraft-cu11/pylibraft_cu11-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (839.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m839.3/839.3 MB\u001b[0m \u001b[31m253.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting raft-dask-cu11==24.10.* (from cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading https://pypi.nvidia.com/raft-dask-cu11/raft_dask_cu11-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (196.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.9/196.9 MB\u001b[0m \u001b[31m266.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting rapids-dask-dependency==24.10.* (from cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading https://pypi.nvidia.com/rapids-dask-dependency/rapids_dask_dependency-24.10.0-py3-none-any.whl (15 kB)\r\n",
      "Collecting scipy>=1.8.0 (from cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m285.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting treelite==4.3.0 (from cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading treelite-4.3.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting click>=8.1 (from dask-cuda==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting pynvml<11.5,>=11.0.0 (from dask-cuda==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading pynvml-11.4.1-py3-none-any.whl.metadata (7.7 kB)\r\n",
      "Collecting zict>=2.0.0 (from dask-cuda==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\r\n",
      "Collecting distributed-ucxx-cu11==0.40.* (from raft-dask-cu11==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading https://pypi.nvidia.com/distributed-ucxx-cu11/distributed_ucxx_cu11-0.40.0-py3-none-any.whl (24 kB)\r\n",
      "Collecting ucx-py-cu11==0.40.* (from raft-dask-cu11==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading https://pypi.nvidia.com/ucx-py-cu11/ucx_py_cu11-0.40.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m258.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting dask==2024.9.0 (from rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading dask-2024.9.0-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Collecting distributed==2024.9.0 (from rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading distributed-2024.9.0-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Collecting dask-expr==1.1.14 (from rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading dask_expr-1.1.14-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting cloudpickle>=3.0.0 (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\r\n",
      "Collecting partd>=1.4.0 (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Collecting pyyaml>=5.3.1 (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\r\n",
      "Collecting toolz>=0.10.0 (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\r\n",
      "Collecting importlib-metadata>=4.13.0 (from dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\r\n",
      "Collecting jinja2>=2.10.3 (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Collecting locket>=1.0.0 (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Collecting msgpack>=1.0.2 (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\r\n",
      "Collecting psutil>=5.8.0 (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\r\n",
      "Collecting sortedcontainers>=2.0.5 (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting tblib>=1.6.0 (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\r\n",
      "Collecting tornado>=6.2.0 (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\r\n",
      "Collecting urllib3>=1.26.5 (from distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Collecting ucxx-cu11==0.40.* (from distributed-ucxx-cu11==0.40.*->raft-dask-cu11==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading https://pypi.nvidia.com/ucxx-cu11/ucxx_cu11-0.40.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (714 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m714.9/714.9 kB\u001b[0m \u001b[31m329.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting libucx-cu11<1.18,>=1.15.0 (from ucx-py-cu11==0.40.*->raft-dask-cu11==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading libucx_cu11-1.17.0-py3-none-manylinux_2_28_x86_64.whl.metadata (2.9 kB)\r\n",
      "Collecting libucxx-cu11==0.40.* (from ucxx-cu11==0.40.*->distributed-ucxx-cu11==0.40.*->raft-dask-cu11==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading https://pypi.nvidia.com/libucxx-cu11/libucxx_cu11-0.40.0-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (503 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.4/503.4 kB\u001b[0m \u001b[31m334.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting fastrlock>=0.5 (from cupy-cuda11x>=12.0.0->cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading fastrlock-0.8.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (9.3 kB)\r\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba>=0.57->cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\r\n",
      "Collecting numpy<3.0a0,>=1.23 (from cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m280.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas<2.2.3dev0,>=2.0->cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\r\n",
      "Collecting pytz>=2020.1 (from pandas<2.2.3dev0,>=2.0->cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Collecting tzdata>=2022.7 (from pandas<2.2.3dev0,>=2.0->cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich->cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas<2.2.3dev0,>=2.0->cudf-cu11->-r requirements.txt (line 1))\r\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=4.13.0->dask==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.10.3->distributed==2024.9.0->rapids-dask-dependency==24.10.*->cuml-cu11->-r requirements.txt (line 2))\r\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\r\n",
      "Downloading treelite-4.3.0-py3-none-manylinux2014_x86_64.whl (915 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m916.0/916.0 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading dask-2024.9.0-py3-none-any.whl (1.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m201.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading dask_expr-1.1.14-py3-none-any.whl (242 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.6/242.6 kB\u001b[0m \u001b[31m347.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading distributed-2024.9.0-py3-none-any.whl (1.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m284.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading cuda_python-11.8.5.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m168.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading cupy_cuda11x-13.3.0-cp310-cp310-manylinux2014_x86_64.whl (96.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 MB\u001b[0m \u001b[31m202.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m327.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m352.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m274.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m256.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvtx-0.2.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (582 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m582.6/582.6 kB\u001b[0m \u001b[31m259.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m242.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m258.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m235.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m280.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\r\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\r\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m331.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m273.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fastrlock-0.8.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (51 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m250.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m261.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m281.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m332.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pynvml-11.4.1-py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m249.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m285.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m352.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m291.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m206.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\r\n",
      "Downloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\r\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m284.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading libucx_cu11-1.17.0-py3-none-manylinux_2_28_x86_64.whl (26.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m163.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\r\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\r\n",
      "Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m231.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading partd-1.4.2-py3-none-any.whl (18 kB)\r\n",
      "Downloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m336.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m279.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading six-1.16.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\r\n",
      "Downloading tblib-3.0.0-py3-none-any.whl (12 kB)\r\n",
      "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m226.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (437 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.2/437.2 kB\u001b[0m \u001b[31m332.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m296.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\r\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\r\n",
      "Saved ./cudf_cu11-24.10.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\r\n",
      "Saved ./libcudf_cu11-24.10.1-py3-none-manylinux_2_28_x86_64.whl\r\n",
      "Saved ./pylibcudf_cu11-24.10.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\r\n",
      "Saved ./rmm_cu11-24.10.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\r\n",
      "Saved ./cuml_cu11-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl\r\n",
      "Saved ./cuvs_cu11-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl\r\n",
      "Saved ./dask_cuda-24.10.0-py3-none-any.whl\r\n",
      "Saved ./dask_cudf_cu11-24.10.1-py3-none-any.whl\r\n",
      "Saved ./pylibraft_cu11-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl\r\n",
      "Saved ./raft_dask_cu11-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl\r\n",
      "Saved ./rapids_dask_dependency-24.10.0-py3-none-any.whl\r\n",
      "Saved ./treelite-4.3.0-py3-none-manylinux2014_x86_64.whl\r\n",
      "Saved ./dask-2024.9.0-py3-none-any.whl\r\n",
      "Saved ./dask_expr-1.1.14-py3-none-any.whl\r\n",
      "Saved ./distributed-2024.9.0-py3-none-any.whl\r\n",
      "Saved ./distributed_ucxx_cu11-0.40.0-py3-none-any.whl\r\n",
      "Saved ./ucx_py_cu11-0.40.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\r\n",
      "Saved ./ucxx_cu11-0.40.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\r\n",
      "Saved ./libucxx_cu11-0.40.0-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\r\n",
      "Saved ./cuda_python-11.8.5.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Saved ./cupy_cuda11x-13.3.0-cp310-cp310-manylinux2014_x86_64.whl\r\n",
      "Saved ./fsspec-2024.10.0-py3-none-any.whl\r\n",
      "Saved ./joblib-1.4.2-py3-none-any.whl\r\n",
      "Saved ./numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\r\n",
      "Saved ./numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Saved ./nvtx-0.2.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Saved ./packaging-24.2-py3-none-any.whl\r\n",
      "Saved ./pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Saved ./pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl\r\n",
      "Saved ./scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Saved ./typing_extensions-4.12.2-py3-none-any.whl\r\n",
      "Saved ./cachetools-5.5.0-py3-none-any.whl\r\n",
      "Saved ./cubinlinker_cu11-0.3.0.post2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Saved ./ptxcompiler_cu11-0.8.1.post2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl\r\n",
      "Saved ./rich-13.9.4-py3-none-any.whl\r\n",
      "Saved ./click-8.1.7-py3-none-any.whl\r\n",
      "Saved ./fastrlock-0.8.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl\r\n",
      "Saved ./llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Saved ./markdown_it_py-3.0.0-py3-none-any.whl\r\n",
      "Saved ./pygments-2.18.0-py3-none-any.whl\r\n",
      "Saved ./pynvml-11.4.1-py3-none-any.whl\r\n",
      "Saved ./python_dateutil-2.9.0.post0-py2.py3-none-any.whl\r\n",
      "Saved ./pytz-2024.2-py2.py3-none-any.whl\r\n",
      "Saved ./tzdata-2024.2-py2.py3-none-any.whl\r\n",
      "Saved ./zict-3.0.0-py2.py3-none-any.whl\r\n",
      "Saved ./cloudpickle-3.1.0-py3-none-any.whl\r\n",
      "Saved ./importlib_metadata-8.5.0-py3-none-any.whl\r\n",
      "Saved ./jinja2-3.1.4-py3-none-any.whl\r\n",
      "Saved ./libucx_cu11-1.17.0-py3-none-manylinux_2_28_x86_64.whl\r\n",
      "Saved ./locket-1.0.0-py2.py3-none-any.whl\r\n",
      "Saved ./mdurl-0.1.2-py3-none-any.whl\r\n",
      "Saved ./msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Saved ./partd-1.4.2-py3-none-any.whl\r\n",
      "Saved ./psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Saved ./PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Saved ./six-1.16.0-py2.py3-none-any.whl\r\n",
      "Saved ./sortedcontainers-2.4.0-py2.py3-none-any.whl\r\n",
      "Saved ./tblib-3.0.0-py3-none-any.whl\r\n",
      "Saved ./toolz-1.0.0-py3-none-any.whl\r\n",
      "Saved ./tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Saved ./urllib3-2.2.3-py3-none-any.whl\r\n",
      "Saved ./MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Saved ./zipp-3.21.0-py3-none-any.whl\r\n",
      "Successfully downloaded cudf-cu11 libcudf-cu11 pylibcudf-cu11 rmm-cu11 cuml-cu11 cuvs-cu11 dask-cuda dask-cudf-cu11 pylibraft-cu11 raft-dask-cu11 rapids-dask-dependency treelite dask dask-expr distributed distributed-ucxx-cu11 ucx-py-cu11 ucxx-cu11 libucxx-cu11 cuda-python cupy-cuda11x fsspec joblib numba numpy nvtx packaging pandas pyarrow scipy typing_extensions cachetools cubinlinker-cu11 ptxcompiler-cu11 rich click fastrlock llvmlite markdown-it-py pygments pynvml python-dateutil pytz tzdata zict cloudpickle importlib-metadata jinja2 libucx-cu11 locket mdurl msgpack partd psutil pyyaml six sortedcontainers tblib toolz tornado urllib3 MarkupSafe zipp\r\n"
     ]
    }
   ],
   "source": [
    "! python -m pip download --no-cache-dir --extra-index-url https://pypi.nvidia.com --destination-directory . -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ebd0539",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:05:20.616477Z",
     "iopub.status.busy": "2024-11-23T06:05:20.616162Z",
     "iopub.status.idle": "2024-11-23T06:05:28.591143Z",
     "shell.execute_reply": "2024-11-23T06:05:28.590284Z"
    },
    "papermill": {
     "duration": 8.007678,
     "end_time": "2024-11-23T06:05:28.593009",
     "exception": false,
     "start_time": "2024-11-23T06:05:20.585331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24.08.00'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cuml\n",
    "cuml.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e78fb23d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:05:28.655614Z",
     "iopub.status.busy": "2024-11-23T06:05:28.655153Z",
     "iopub.status.idle": "2024-11-23T06:07:05.634761Z",
     "shell.execute_reply": "2024-11-23T06:07:05.633862Z"
    },
    "papermill": {
     "duration": 97.012395,
     "end_time": "2024-11-23T06:07:05.636889",
     "exception": false,
     "start_time": "2024-11-23T06:05:28.624494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy-langdetect\r\n",
      "  Downloading spacy_langdetect-0.1.2-py3-none-any.whl.metadata (3.4 kB)\r\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.10/site-packages (from spacy-langdetect) (8.3.3)\r\n",
      "Collecting langdetect==1.0.7 (from spacy-langdetect)\r\n",
      "  Downloading langdetect-1.0.7.zip (998 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m998.1/998.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect==1.0.7->spacy-langdetect) (1.16.0)\r\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest->spacy-langdetect) (2.0.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pytest->spacy-langdetect) (21.3)\r\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/conda/lib/python3.10/site-packages (from pytest->spacy-langdetect) (1.5.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest->spacy-langdetect) (1.2.0)\r\n",
      "Requirement already satisfied: tomli>=1 in /opt/conda/lib/python3.10/site-packages (from pytest->spacy-langdetect) (2.0.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pytest->spacy-langdetect) (3.1.2)\r\n",
      "Downloading spacy_langdetect-0.1.2-py3-none-any.whl (5.0 kB)\r\n",
      "Building wheels for collected packages: langdetect\r\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.7-py3-none-any.whl size=993414 sha256=974abbf871bb51409b105fa39b5dfc39964c08f1f5b7dd3296fe2b618b6dfe9f\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/97/f1/e4/8b73f7a0421b132755956892d29b1e764d3e0857a6e92e32fe\r\n",
      "Successfully built langdetect\r\n",
      "Installing collected packages: langdetect, spacy-langdetect\r\n",
      "Successfully installed langdetect-1.0.7 spacy-langdetect-0.1.2\r\n",
      "Collecting language-detector\r\n",
      "  Downloading language-detector-5.0.2.tar.gz (6.6 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: language-detector\r\n",
      "  Building wheel for language-detector (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for language-detector: filename=language_detector-5.0.2-py3-none-any.whl size=7029 sha256=8b52f34bc6c199782346a13b8440fbe1529ad1dc31973b352b7d4d241a1548f7\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/02/dc/f0/85ab33b10b9ada1852ac83ba55b87088678d3cef9017d98104\r\n",
      "Successfully built language-detector\r\n",
      "Installing collected packages: language-detector\r\n",
      "Successfully installed language-detector-5.0.2\r\n",
      "Collecting symspellpy\r\n",
      "  Downloading symspellpy-6.7.8-py3-none-any.whl.metadata (3.9 kB)\r\n",
      "Collecting editdistpy>=0.1.3 (from symspellpy)\r\n",
      "  Downloading editdistpy-0.1.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\r\n",
      "Downloading symspellpy-6.7.8-py3-none-any.whl (2.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading editdistpy-0.1.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.1/144.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: editdistpy, symspellpy\r\n",
      "Successfully installed editdistpy-0.1.5 symspellpy-6.7.8\r\n",
      "Collecting sentence-transformers\r\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\r\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\r\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: sentence-transformers\r\n",
      "Successfully installed sentence-transformers-3.3.1\r\n",
      "Requirement already satisfied: langdetect in /opt/conda/lib/python3.10/site-packages (1.0.7)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect) (1.16.0)\r\n",
      "Collecting umap-learn\r\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (1.14.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (1.2.2)\r\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.10/site-packages (from umap-learn) (0.60.0)\r\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\r\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from umap-learn) (4.66.4)\r\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.43.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\r\n",
      "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pynndescent, umap-learn\r\n",
      "Successfully installed pynndescent-0.5.13 umap-learn-0.5.7\r\n",
      "Collecting stop_words\r\n",
      "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: stop_words\r\n",
      "  Building wheel for stop_words (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32895 sha256=10bdb416b871db2fe0e0b082c1e1c5334695653fbad1db5cced872749b76927f\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/d0/1a/23/f12552a50cb09bcc1694a5ebb6c2cd5f2a0311de2b8c3d9a89\r\n",
      "Successfully built stop_words\r\n",
      "Installing collected packages: stop_words\r\n",
      "Successfully installed stop_words-2018.7.23\r\n",
      "Collecting jolib\r\n",
      "  Downloading joLib-0.0.1-py3-none-any.whl.metadata (554 bytes)\r\n",
      "Downloading joLib-0.0.1-py3-none-any.whl (1.2 kB)\r\n",
      "Installing collected packages: jolib\r\n",
      "Successfully installed jolib-0.0.1\r\n",
      "Collecting faiss-gpu\r\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n",
      "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\r\n",
      "Successfully installed faiss-gpu-1.7.2\r\n",
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\r\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy-langdetect\n",
    "!pip install language-detector\n",
    "!pip install symspellpy\n",
    "# !pip install dask distributed dask-cuda\n",
    "!pip install sentence-transformers\n",
    "!pip install langdetect\n",
    "!pip install umap-learn\n",
    "!pip install stop_words\n",
    "# !pip install hdbscan\n",
    "!pip install jolib\n",
    "!pip install faiss-gpu\n",
    "!pip install torchsummary\n",
    "# !pip install cuml-cu11 --extra-index-url=https://pypi.nvidia.com\n",
    "# !conda create -n rapids-24.10 -c rapidsai -c conda-forge -c nvidia  \\\n",
    "#     rapids=24.10 python=3.10 'cuda-version>=12.0,<=12.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5caec2ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:05.707787Z",
     "iopub.status.busy": "2024-11-23T06:07:05.707149Z",
     "iopub.status.idle": "2024-11-23T06:07:05.711362Z",
     "shell.execute_reply": "2024-11-23T06:07:05.710716Z"
    },
    "papermill": {
     "duration": 0.040956,
     "end_time": "2024-11-23T06:07:05.712930",
     "exception": false,
     "start_time": "2024-11-23T06:07:05.671974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !cp ../input/rapids/rapids.21.06 /opt/conda/envs/rapids.tar.gz\n",
    "# !cd /opt/conda/envs/ && tar -xzvf rapids.tar.gz > /dev/null\n",
    "# sys.path = [\"/opt/conda/envs/rapids/lib/python3.7/site-packages\"] + sys.path\n",
    "# sys.path = [\"/opt/conda/envs/rapids/lib/python3.7\"] + sys.path\n",
    "# sys.path = [\"/opt/conda/envs/rapids/lib\"] + sys.path \n",
    "# !cp /opt/conda/envs/rapids/lib/libxgboost.so /opt/conda/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3a4d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:05.781986Z",
     "iopub.status.busy": "2024-11-23T06:07:05.781754Z",
     "iopub.status.idle": "2024-11-23T06:07:05.785538Z",
     "shell.execute_reply": "2024-11-23T06:07:05.784740Z"
    },
    "papermill": {
     "duration": 0.040469,
     "end_time": "2024-11-23T06:07:05.787279",
     "exception": false,
     "start_time": "2024-11-23T06:07:05.746810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"cuML Version: \", cuml.__version__)\n",
    "\n",
    "# !docker run \\\n",
    "#     --rm \\\n",
    "#     -it \\\n",
    "#     --pull always \\\n",
    "#     --gpus all \\\n",
    "#     -shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 \\\n",
    "#     -e EXTRA_CONDA_PACKAGES=\"jq\" \\\n",
    "#     -e EXTRA_PIP_PACKAGES=\"beautifulsoup4\" \\\n",
    "#     -p 8888:8888 \\\n",
    "#     rapidsai/notebooks:24.10-cuda12.5-py3.12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce667e28",
   "metadata": {
    "papermill": {
     "duration": 0.033793,
     "end_time": "2024-11-23T06:07:05.854996",
     "exception": false,
     "start_time": "2024-11-23T06:07:05.821203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b576910",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:05.924026Z",
     "iopub.status.busy": "2024-11-23T06:07:05.923747Z",
     "iopub.status.idle": "2024-11-23T06:07:34.630853Z",
     "shell.execute_reply": "2024-11-23T06:07:34.629901Z"
    },
    "papermill": {
     "duration": 28.743753,
     "end_time": "2024-11-23T06:07:34.632676",
     "exception": false,
     "start_time": "2024-11-23T06:07:05.888923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/faiss/loader.py:28: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(numpy.__version__) >= \"1.19\":\n",
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/tmp/ipykernel_23/2671139526.py:40: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2309: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(parent)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.iam')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2309: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(parent)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/opt/conda/lib/python3.10/site-packages/nltk/decorators.py:69: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  signature = inspect.formatargspec(regargs, varargs, varkwargs, defaults,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "from langdetect import detect\n",
    "import re\n",
    "import argparse\n",
    "import concurrent.futures\n",
    "from multiprocessing import Lock\n",
    "import faiss\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from hdbscan import HDBSCAN\n",
    "from cuml.cluster import HDBSCAN\n",
    "# from dask_cuda import LocalCUDACluster\n",
    "# import dask.array as da\n",
    "# from dask.distributed import Client\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "#from geotext import GeoText\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "from collections import Counter\n",
    "# import umap.umap_ as umap\n",
    "from cuml import UMAP\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pkg_resources\n",
    "from joblib import dump, load\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from stop_words import get_stop_words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from language_detector import detect_language\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# import keras\n",
    "# from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "# from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# import keras.losses\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ce5f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:34.707535Z",
     "iopub.status.busy": "2024-11-23T06:07:34.706324Z",
     "iopub.status.idle": "2024-11-23T06:07:34.715657Z",
     "shell.execute_reply": "2024-11-23T06:07:34.714867Z"
    },
    "papermill": {
     "duration": 0.048073,
     "end_time": "2024-11-23T06:07:34.717352",
     "exception": false,
     "start_time": "2024-11-23T06:07:34.669279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "files_path = '/kaggle/input'\n",
    "\n",
    "load_existing_model = False #@param {type:\"boolean\"}\n",
    "load_existing_preprocessed_data = False #@param {type:\"boolean\"}\n",
    "dataset_file_path = f'{files_path}/song-lyrics-filtered-seven-hundred-mb'\n",
    "\n",
    "checkpoints_input_path = f'{files_path}/preprocessed_data_checkpoints/other/default/4'\n",
    "val_checkpoints_input_path = f'{files_path}/song-recommendation-val-inputs/other/default/4'\n",
    "\n",
    "tfidf_input_path = f'{files_path}/song-recommendation-tf-idf/other/default/2'\n",
    "val_tfidf_input_path = f'{files_path}/val-song-recommendation-tf-idf/other/default/3'\n",
    "\n",
    "lda_input_path = f'{files_path}/song-recommendation-lda/other/default/4'\n",
    "val_lda_input_path = f'{files_path}/val-song-recommendation-lda/other/default/3'\n",
    "\n",
    "song_embeddings_input_path = f'{files_path}/song-embeddings-mpnet/other/default/2'\n",
    "val_song_embeddings_input_path = f'{files_path}/val-song-embeddings-mpnet/other/default/1'\n",
    "\n",
    "autoencoder_input_path = f'{files_path}/song-recommendation-autoencoder/other/default/2'\n",
    "val_autoencoder_input_path = f'{files_path}/val-song-recommendation-autoencoder/other/default/1'\n",
    "\n",
    "autoencoder_input_path_24_dim = f'{files_path}/song-recommendation-autoencoder-24-dim/other/default/1'\n",
    "val_autoencoder_input_path_24_dim = f'{files_path}/val-song-recommendation-autoencoder-24-dim/other/default/1'\n",
    "\n",
    "song_recommender_cache_input_path = f'{files_path}/song-recommender-cache/other/default/2'\n",
    "reordered_lyrics_input_path = f'{files_path}/song-recommendation-re-ordered-lyrics/other/default/1/reordered_lyrics.pkl'\n",
    "\n",
    "load_preprocessed_checkpoints = True\n",
    "working_files_path = '/kaggle/working'\n",
    "model_files_path = 'models'\n",
    "vector_files_path = 'vectors'\n",
    "song_recommender_cache_out_path = 'song_recommender_cache'\n",
    "out_checkpoints_path = 'checkpoints'\n",
    "\n",
    "autoencoder_checkpoint_dir = f'{vector_files_path}/autoencoder'\n",
    "two_d_vis_path = '2d_vis'\n",
    "wordcloud_path = 'wordclouds'\n",
    "data_file_name = 'song_lyrics_filtered_seven_hundred_mb' #@param {type:\"string\"}\n",
    "method = \"LDA_BERT\" #@param {type:\"string\"}\n",
    "ntopic = 15 #@param {type:\"integer\"}\n",
    "data_file = f'{data_file_name}.csv'\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "directories = [model_files_path, vector_files_path, out_checkpoints_path, two_d_vis_path, wordcloud_path, autoencoder_checkpoint_dir, song_recommender_cache_out_path]\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78b43fd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:34.791263Z",
     "iopub.status.busy": "2024-11-23T06:07:34.790447Z",
     "iopub.status.idle": "2024-11-23T06:07:50.238564Z",
     "shell.execute_reply": "2024-11-23T06:07:50.237854Z"
    },
    "papermill": {
     "duration": 15.486975,
     "end_time": "2024-11-23T06:07:50.240606",
     "exception": false,
     "start_time": "2024-11-23T06:07:34.753631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not load_existing_preprocessed_data:\n",
    "    meta = pd.read_csv(f'{dataset_file_path}/{data_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87c98d1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:50.319967Z",
     "iopub.status.busy": "2024-11-23T06:07:50.319274Z",
     "iopub.status.idle": "2024-11-23T06:07:50.365723Z",
     "shell.execute_reply": "2024-11-23T06:07:50.364990Z"
    },
    "papermill": {
     "duration": 0.087456,
     "end_time": "2024-11-23T06:07:50.367673",
     "exception": false,
     "start_time": "2024-11-23T06:07:50.280217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not load_existing_preprocessed_data:\n",
    "    ##extract the lyrics to pandas\n",
    "    documents = meta[['lyrics']]\n",
    "    documents.reset_index(inplace = True)\n",
    "\n",
    "    ##create pandas data frame with all lyrics, use as input corpus\n",
    "    documents[\"index\"] = documents.index.values\n",
    "    documents.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89203d2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:50.444990Z",
     "iopub.status.busy": "2024-11-23T06:07:50.444640Z",
     "iopub.status.idle": "2024-11-23T06:07:50.448883Z",
     "shell.execute_reply": "2024-11-23T06:07:50.448167Z"
    },
    "papermill": {
     "duration": 0.044913,
     "end_time": "2024-11-23T06:07:50.450578",
     "exception": false,
     "start_time": "2024-11-23T06:07:50.405665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not load_existing_preprocessed_data:\n",
    "    documents.head(15)\n",
    "    # db_scan_min_samples = len(documents) / 50\n",
    "    # print(db_scan_min_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879cdbf",
   "metadata": {
    "papermill": {
     "duration": 0.034532,
     "end_time": "2024-11-23T06:07:50.559121",
     "exception": false,
     "start_time": "2024-11-23T06:07:50.524589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cee5079",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:50.630428Z",
     "iopub.status.busy": "2024-11-23T06:07:50.629626Z",
     "iopub.status.idle": "2024-11-23T06:07:50.644961Z",
     "shell.execute_reply": "2024-11-23T06:07:50.644218Z"
    },
    "papermill": {
     "duration": 0.052665,
     "end_time": "2024-11-23T06:07:50.646603",
     "exception": false,
     "start_time": "2024-11-23T06:07:50.593938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_topic_words(token_lists, labels, k=None):\n",
    "    \"\"\"\n",
    "    get top words within each topic from clustering results\n",
    "    \"\"\"\n",
    "    if k is None:\n",
    "        k = len(np.unique(labels))\n",
    "    topics = ['' for _ in range(k)]\n",
    "    for i, c in enumerate(token_lists):\n",
    "        topics[labels[i]] += (' ' + ' '.join(c))\n",
    "    word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n",
    "    # get sorted word counts\n",
    "    word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True), word_counts))\n",
    "    # get topics\n",
    "    topics = list(map(lambda x: list(map(lambda x: x[0], x[:10])), word_counts))\n",
    "\n",
    "    return topics\n",
    "\n",
    "def get_coherence(model, token_lists, measure='c_v'):\n",
    "    \"\"\"\n",
    "    Get model coherence from gensim.models.coherencemodel\n",
    "    :param model: TopicModel object\n",
    "    :param token_lists: token lists of docs\n",
    "    :param topics: topics as top words\n",
    "    :param measure: coherence metrics\n",
    "    :return: coherence score\n",
    "    \"\"\"\n",
    "    topics = get_topic_words(token_lists, model.cluster_model.labels_)\n",
    "    cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
    "                        coherence=measure)\n",
    "    return cm.get_coherence()\n",
    "\n",
    "def get_silhouette(model):\n",
    "    \"\"\"\n",
    "    Get silhouette score from model\n",
    "    :param model: TopicModel object\n",
    "    :return: silhouette score\n",
    "    \"\"\"\n",
    "    lbs = model.cluster_model.labels_\n",
    "    vec = model.vec['BERT_LDA']\n",
    "    return silhouette_score(vec, lbs)\n",
    "\n",
    "def plot_proj(embedding, lbs):\n",
    "    \"\"\"\n",
    "    Plot UMAP embeddings\n",
    "    :param embedding: UMAP (or other) embeddings\n",
    "    :param lbs: labels\n",
    "    \"\"\"\n",
    "    n = len(embedding)\n",
    "    counter = Counter(lbs)\n",
    "    for i in range(len(np.unique(lbs))):\n",
    "        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n",
    "                 label='cluster {}: {:.2f}%'.format(i, counter[i] / n * 100))\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.grid(color ='grey', linestyle='-',linewidth = 0.25)\n",
    "\n",
    "\n",
    "def visualize(model):\n",
    "    \"\"\"\n",
    "    Visualize the result for the topic model by 2D embedding (UMAP)\n",
    "    :param model: TopicModel object\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        return\n",
    "    reducer = umap.UMAP()\n",
    "    print('Calculating UMAP projection ...')\n",
    "    vec_umap = reducer.fit_transform(model.vec[model.method])\n",
    "    print('Calculating UMAP projection. Done!')\n",
    "    # Retrieve cluster labels\n",
    "    if hasattr(model.cluster_model, 'labels_'):\n",
    "        # For KMeans or any clustering model with labels_ attribute\n",
    "        labels = model.cluster_model.labels_\n",
    "    else:\n",
    "        # For Agglomerative Clustering or models that require fit_predict\n",
    "        labels = model.cluster_model.fit_predict(model.vec[model.method])\n",
    "    plot_proj(vec_umap, labels)\n",
    "    # dr = '{}/contextual_topic_identification/docs/images/{}/{}'.format(working_files_path, model.method, model.id)\n",
    "    # if not os.path.exists(dr):\n",
    "    #     os.makedirs(dr)\n",
    "    plt.savefig(f'{two_d_vis_path}/{model.method}/{model.id}')\n",
    "    # Visualizing the dendrogram for Agglomerative Clustering\n",
    "    if isinstance(model.cluster_model, AgglomerativeClustering):\n",
    "        print(\"Calculating and plotting dendrogram ...\")\n",
    "\n",
    "        # Compute the linkage matrix using the vectorized embeddings\n",
    "        # You can use different methods like 'ward', 'complete', 'average', etc. for linkage\n",
    "        Z = linkage(model.vec[model.method], method='ward')\n",
    "\n",
    "        # Plot the dendrogram\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        dendrogram(Z)\n",
    "        plt.title('Dendrogram for Hierarchical Clustering')\n",
    "        plt.xlabel('Sample index')\n",
    "        plt.ylabel('Distance')\n",
    "\n",
    "        # Save the dendrogram plot\n",
    "        plt.savefig(f'{two_d_vis_path}/{model.method}/{model.id}_dendrogram')\n",
    "        print(\"Dendrogram plot saved!\")\n",
    "\n",
    "\n",
    "def get_wordcloud(model, token_lists, topic):\n",
    "    \"\"\"\n",
    "    Get word cloud of each topic from fitted model\n",
    "    :param model: TopicModel object\n",
    "    :param sentences: preprocessed sentences from docs\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        return\n",
    "    print('Getting wordcloud for topic {} ...'.format(topic))\n",
    "    lbs = model.cluster_model.labels_\n",
    "    # tokens = ' '.join([' '.join(_) for _ in np.array(token_lists)[lbs == topic]])\n",
    "    tokens = ' '.join([' '.join(token_lists[i]) for i in range(len(token_lists)) if lbs[i] == topic])\n",
    "\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=560,\n",
    "                          background_color='white', collocations=False,\n",
    "                          min_font_size=10).generate(tokens)\n",
    "\n",
    "    # plot the WordCloud image\n",
    "    plt.figure(figsize=(8, 5.6), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    dr = '{}/{}/{}'.format(wordcloud_path, model.method, model.id)\n",
    "    if not os.path.exists(dr):\n",
    "        os.makedirs(dr)\n",
    "    plt.savefig(f'{dr}/Topic_{topic}_wordcloud')\n",
    "    print('Getting wordcloud for topic {}. Done!'.format(topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd1b2a1",
   "metadata": {
    "papermill": {
     "duration": 0.036598,
     "end_time": "2024-11-23T06:07:50.718821",
     "exception": false,
     "start_time": "2024-11-23T06:07:50.682223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c98051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:50.792219Z",
     "iopub.status.busy": "2024-11-23T06:07:50.791883Z",
     "iopub.status.idle": "2024-11-23T06:07:55.958664Z",
     "shell.execute_reply": "2024-11-23T06:07:55.957934Z"
    },
    "papermill": {
     "duration": 5.206507,
     "end_time": "2024-11-23T06:07:55.960634",
     "exception": false,
     "start_time": "2024-11-23T06:07:50.754127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not load_existing_preprocessed_data:\n",
    "    sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "        \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "    if sym_spell.word_count:\n",
    "        pass\n",
    "    else:\n",
    "        sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "\n",
    "###################################\n",
    "#### sentence level preprocess ####\n",
    "###################################\n",
    "\n",
    "# lowercase + base filter\n",
    "# some basic normalization\n",
    "def f_base(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: processed string: see comments in the source code for more info\n",
    "    \"\"\"\n",
    "    # normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n",
    "    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n",
    "    # normalization 2: lower case\n",
    "    s = s.lower()\n",
    "    # normalization 3: \"&gt\", \"&lt\"\n",
    "    s = re.sub(r'&gt|&lt', ' ', s)\n",
    "    # normalization 4: letter repetition (if more than 2)\n",
    "    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
    "    # normalization 5: non-word repetition (if more than 1)\n",
    "    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
    "    # normalization 6: string * as delimiter\n",
    "    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n",
    "    # normalization 7: stuff in parenthesis, assumed to be less informal\n",
    "    s = re.sub(r'\\(.*?\\)', '. ', s)\n",
    "    # normalization 8: xxx[?!]. -- > xxx.\n",
    "    s = re.sub(r'\\W+?\\.', '.', s)\n",
    "    # normalization 9: [.?!] --> [.?!] xxx\n",
    "    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n",
    "    # normalization 10: ' ing ', noise text\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "    # normalization 11: noise text\n",
    "    s = re.sub(r'product received for free[.| ]', ' ', s)\n",
    "    # normalization 12: phrase repetition\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "# language detection\n",
    "def f_lan(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: boolean (s is English)\n",
    "    \"\"\"\n",
    "\n",
    "    # some reviews are actually english but biased toward french\n",
    "    return detect_language(s) in {'English', 'French','Spanish','Chinese'}\n",
    "\n",
    "\n",
    "###############################\n",
    "#### word level preprocess ####\n",
    "###############################\n",
    "\n",
    "# filtering out punctuations and numbers\n",
    "def f_punct(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with punct and number filter out\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word.isalpha()]\n",
    "\n",
    "\n",
    "# selecting nouns\n",
    "def f_noun(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with only nouns selected\n",
    "    \"\"\"\n",
    "    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n",
    "\n",
    "def f_pos_tags(w_list):\n",
    "    pos_tags = pos_tag(w_list)\n",
    "    selected_pos_tags = ['NN', 'NNS',  # Nouns\n",
    "                         'VB', 'VBD', 'VBG', 'VBN', #'VBP', 'VBZ',  # Verbs\n",
    "                         'JJ', 'JJR', 'JJS']  # Adjectives\n",
    "    filtered_words = [word for word, pos in pos_tags if pos in selected_pos_tags]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "\n",
    "# typo correction\n",
    "def f_typo(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with typo fixed by symspell. words with no match up will be dropped\n",
    "    \"\"\"\n",
    "    w_list_fixed = []\n",
    "    for word in w_list:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "        if suggestions:\n",
    "            w_list_fixed.append(suggestions[0].term)\n",
    "        else:\n",
    "            pass\n",
    "            # do word segmentation, deprecated for inefficiency\n",
    "            # w_seg = sym_spell.word_segmentation(phrase=word)\n",
    "            # w_list_fixed.extend(w_seg.corrected_string.split())\n",
    "    return w_list_fixed\n",
    "\n",
    "\n",
    "# stemming if doing word-wise\n",
    "if not load_existing_preprocessed_data:\n",
    "    # p_stemmer = PorterStemmer()\n",
    "    s_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def f_stem(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with stemming\n",
    "    \"\"\"\n",
    "    return [s_stemmer.stem(word) for word in w_list]\n",
    "\n",
    "\n",
    "# filtering out stop words\n",
    "# create English stop words list\n",
    "if not load_existing_preprocessed_data:\n",
    "    stop_words = (list(\n",
    "        set(get_stop_words('en'))\n",
    "        |set(get_stop_words('es'))\n",
    "        |set(get_stop_words('de'))\n",
    "        |set(get_stop_words('it'))\n",
    "        |set(get_stop_words('ca'))\n",
    "        #|set(get_stop_words('cy'))\n",
    "        |set(get_stop_words('pt'))\n",
    "        #|set(get_stop_words('tl'))\n",
    "        |set(get_stop_words('pl'))\n",
    "        #|set(get_stop_words('et'))\n",
    "        |set(get_stop_words('da'))\n",
    "        |set(get_stop_words('ru'))\n",
    "        #|set(get_stop_words('so'))\n",
    "        |set(get_stop_words('sv'))\n",
    "        |set(get_stop_words('sk'))\n",
    "        #|set(get_stop_words('cs'))\n",
    "        |set(get_stop_words('nl'))\n",
    "        #|set(get_stop_words('sl'))\n",
    "        #|set(get_stop_words('no'))\n",
    "        #|set(get_stop_words('zh-cn'))\n",
    "    ))\n",
    "\n",
    "\n",
    "def f_stopw(w_list):\n",
    "    \"\"\"\n",
    "    filtering out stop words\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word not in stop_words]\n",
    "\n",
    "\n",
    "def preprocess_sent(rw):\n",
    "    \"\"\"\n",
    "    Get sentence level preprocessed data from raw review texts\n",
    "    :param rw: review to be processed\n",
    "    :return: sentence level pre-processed review\n",
    "    \"\"\"\n",
    "    s = f_base(rw)\n",
    "    if not f_lan(s):\n",
    "        return None\n",
    "    return s\n",
    "\n",
    "\n",
    "def preprocess_word(s):\n",
    "    \"\"\"\n",
    "    Get word level preprocessed data from preprocessed sentences\n",
    "    including: remove punctuation, select noun, fix typo, stem, stop_words\n",
    "    :param s: sentence to be processed\n",
    "    :return: word level pre-processed review\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    w_list = word_tokenize(s)\n",
    "    w_list = f_punct(w_list)\n",
    "    # w_list = f_noun(w_list)\n",
    "    w_list = f_pos_tags(w_list)\n",
    "    # w_list = f_typo(w_list)\n",
    "    w_list = f_stem(w_list)\n",
    "    w_list = f_stopw(w_list)\n",
    "\n",
    "    return w_list\n",
    "\n",
    "# def vis_umap_tsne(vec_scaled, labels):\n",
    "#     reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "#     vec_2d = reducer.fit_transform(vec_scaled)\n",
    "\n",
    "#     # Plot the clusters\n",
    "#     plt.scatter(vec_2d[:, 0], vec_2d[:, 1], c=labels, cmap='Spectral', s=5)\n",
    "#     plt.colorbar(label='Cluster Label')\n",
    "#     plt.title('UMAP Projection of Clusters (DBSCAN)')\n",
    "#     plt.show()\n",
    "\n",
    "# def cluster_size_distribution(labels):\n",
    "#     unique, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "#     # Plot the cluster sizes\n",
    "#     sns.barplot(x=unique, y=counts)\n",
    "#     plt.xlabel('Cluster Label')\n",
    "#     plt.ylabel('Number of Points')\n",
    "#     plt.title('Cluster Size Distribution')\n",
    "#     plt.show()\n",
    "\n",
    "# def db_scan_noise(vec_2d, labels):\n",
    "#     noise_mask = labels == -1\n",
    "#     plt.scatter(vec_2d[noise_mask, 0], vec_2d[noise_mask, 1], c='red', s=5)\n",
    "#     plt.title('Noise Points Identified by DBSCAN')\n",
    "#     plt.show()\n",
    "\n",
    "# def find_k_clusters_elbow(vec_scaled):\n",
    "#     # Calculate the distortion for a range of K values\n",
    "#     distortions = []\n",
    "#     K = range(1, 20)  # You can adjust this range as needed\n",
    "\n",
    "#     for k in K:\n",
    "#         kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#         kmeans.fit(vec_scaled)\n",
    "#         distortions.append(kmeans.inertia_)\n",
    "\n",
    "#     # Plot the results\n",
    "#     plt.figure()\n",
    "#     plt.plot(K, distortions, marker='o')\n",
    "#     plt.xlabel('Number of Clusters K')\n",
    "#     plt.ylabel('Distortion (Inertia)')\n",
    "#     plt.title('Elbow Method for Optimal K')\n",
    "#     plt.show()\n",
    "\n",
    "# def find_k_clusters_silhouette(vec_scaled):\n",
    "#     K = range(1, 20)  # You can adjust this range as needed\n",
    "#     silhouette_scores = []\n",
    "\n",
    "#     for k in K[1:]:  # Start from 2 to avoid silhouette score being undefined for 1 cluster\n",
    "#         kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#         labels = kmeans.fit_predict(vec_scaled)\n",
    "#         silhouette_scores.append(silhouette_score(vec_scaled, labels))\n",
    "\n",
    "#     # Plot silhouette scores\n",
    "#     plt.figure()\n",
    "#     plt.plot(K[1:], silhouette_scores, marker='o')\n",
    "#     plt.xlabel('Number of Clusters K')\n",
    "#     plt.ylabel('Silhouette Score')\n",
    "#     plt.title('Silhouette Analysis for Optimal K')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8db37e4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:56.034421Z",
     "iopub.status.busy": "2024-11-23T06:07:56.034107Z",
     "iopub.status.idle": "2024-11-23T06:07:56.054667Z",
     "shell.execute_reply": "2024-11-23T06:07:56.053975Z"
    },
    "papermill": {
     "duration": 0.059302,
     "end_time": "2024-11-23T06:07:56.056228",
     "exception": false,
     "start_time": "2024-11-23T06:07:55.996926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lock = Lock()\n",
    "\n",
    "sentences_input_checkpoint_file = f'{checkpoints_input_path}/checkpoint_sentences.pkl'\n",
    "token_lists_input_checkpoint_file = f'{checkpoints_input_path}/checkpoint_token_lists.pkl'\n",
    "indices_input_checkpoint_file = f'{checkpoints_input_path}/checkpoint_indices.pkl'\n",
    "ngrams_input_checkpoint_file = f'{checkpoints_input_path}/n_grams_n_grams_token_lists.file'\n",
    "\n",
    "def save_progress(sentences, token_lists, idx_in, batch_idx, batch_freq = 200):\n",
    "    file_index = batch_idx // batch_freq\n",
    "    sentences_out_checkpoint_file = f'{out_checkpoints_path}/checkpoint_sentences_{file_index}.pkl'\n",
    "    token_lists_out_checkpoint_file = f'{out_checkpoints_path}/checkpoint_token_lists_{file_index}.pkl'\n",
    "    indices_out_checkpoint_file = f'{out_checkpoints_path}/checkpoint_indices_{file_index}.pkl'\n",
    "    try:\n",
    "        with open(sentences_out_checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(sentences, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(token_lists_out_checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(token_lists, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(indices_out_checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(idx_in, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    except (IOError, pickle.PickleError) as e:\n",
    "        print(f\"Error saving progress: {e}\")\n",
    "\n",
    "def load_progress(\n",
    "        checkpoints_in_path = checkpoints_input_path,\n",
    "        only_load_idx = False):\n",
    "    sentences_input_checkpoint_file = f'{checkpoints_in_path}/checkpoint_sentences.pkl'\n",
    "    token_lists_input_checkpoint_file = f'{checkpoints_in_path}/checkpoint_token_lists.pkl'\n",
    "    indices_input_checkpoint_file = f'{checkpoints_in_path}/checkpoint_indices.pkl'\n",
    "    ngrams_input_checkpoint_file = f'{checkpoints_in_path}/n_grams_n_grams_token_lists.file'\n",
    "    sentences, token_lists, idx_in, ngrams_token_lists = [], [], [], []\n",
    "    if not only_load_idx:\n",
    "        # print('Reading sentences and tokens from checkpoint files - ', sentences_input_checkpoint_file, token_lists_input_checkpoint_file)\n",
    "        # print('Checkpoints exist - ', os.path.exists(sentences_input_checkpoint_file), os.path.exists(token_lists_input_checkpoint_file))\n",
    "        # print('Checkpoints size - ', os.path.getsize(sentences_input_checkpoint_file), os.path.getsize(token_lists_input_checkpoint_file))\n",
    "        if os.path.exists(sentences_input_checkpoint_file) and os.path.getsize(sentences_input_checkpoint_file) > 0:\n",
    "            print('Reading sentences_input_checkpoint_file')\n",
    "            with open(sentences_input_checkpoint_file, 'rb') as f:\n",
    "                sentences = pickle.load(f)\n",
    "        if os.path.exists(token_lists_input_checkpoint_file) and os.path.getsize(token_lists_input_checkpoint_file) > 0:\n",
    "            print('Reading token_lists_input_checkpoint_file')\n",
    "            with open(token_lists_input_checkpoint_file, 'rb') as f:\n",
    "                token_lists = pickle.load(f)\n",
    "        if os.path.exists(ngrams_input_checkpoint_file) and os.path.getsize(ngrams_input_checkpoint_file) > 0:\n",
    "            print('Reading ngrams_input_checkpoint_file')\n",
    "            with open(ngrams_input_checkpoint_file, 'rb') as f:\n",
    "                ngrams_token_lists = pickle.load(f)\n",
    "    if os.path.exists(indices_input_checkpoint_file) and os.path.getsize(indices_input_checkpoint_file) > 0:\n",
    "        print('Reading indices_input_checkpoint_file')\n",
    "        with open(indices_input_checkpoint_file, 'rb') as f:\n",
    "            idx_in = pickle.load(f)\n",
    "    return sentences, token_lists, idx_in, ngrams_token_lists\n",
    "\n",
    "def process_document(doc):\n",
    "    sentence = preprocess_sent(doc['text'])\n",
    "    token_list = preprocess_word(sentence)\n",
    "    return (sentence, token_list, doc['index']) if token_list else (None, None, None)\n",
    "\n",
    "def preprocess(docs, batch_size=100):\n",
    "    # Load previous progress\n",
    "    if load_preprocessed_checkpoints:\n",
    "        print('Loading preprocessed checkpoints')\n",
    "        sentences, token_lists, idx_in = load_progress()\n",
    "        print('Loaded preprocessed checkpoints')\n",
    "    else:\n",
    "        sentences, token_lists, idx_in = [], [], []\n",
    "\n",
    "    start_index = max(idx_in, default=-1) + 1 \n",
    "    total_docs = len(docs)\n",
    "    total_batches = (total_docs - start_index + batch_size - 1) // batch_size\n",
    "\n",
    "    for batch_number in range(total_batches):\n",
    "        start_idx = start_index + batch_number * batch_size\n",
    "        end_idx = min(start_idx + batch_size, total_docs)\n",
    "        batch_docs = [{'index': i, 'text': docs[i]} for i in range(start_idx, end_idx)]\n",
    "\n",
    "        for doc in batch_docs:\n",
    "            sentence = preprocess_sent(doc['text'])\n",
    "            token_list = preprocess_word(sentence)\n",
    "            if token_list:\n",
    "                sentences.append(sentence)\n",
    "                token_lists.append(token_list)\n",
    "                idx_in.append(doc['index'])\n",
    "\n",
    "\n",
    "        # Save progress after each batch\n",
    "        save_progress(sentences, token_lists, idx_in, batch_number + 1)\n",
    "        print(f\"Processed batch {batch_number + 1}/{total_batches}\")\n",
    "\n",
    "    print(\"Preprocessing complete!\")\n",
    "    return sentences, token_lists, idx_in\n",
    "\n",
    "def load_preprocessed_files(preprocessed_sen_files_path = sentences_input_checkpoint_file, preprocessed_words_files_path = token_lists_input_checkpoint_file):\n",
    "    \"\"\"\n",
    "    Load the preprocessed data files from separate checkpoint files.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    token_lists = []\n",
    "\n",
    "    # Attempt to load sentences from its respective checkpoint file\n",
    "    try:\n",
    "        with open(preprocessed_sen_files_path, \"rb\") as f:\n",
    "            sentences = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Sentences file not found. Loading skipped.\")\n",
    "\n",
    "    # Attempt to load token lists from its respective checkpoint file\n",
    "    try:\n",
    "        with open(preprocessed_words_files_path, \"rb\") as f:\n",
    "            token_lists = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Token lists file not found. Loading skipped.\")\n",
    "\n",
    "    print(\"Loaded preprocessed files.\")\n",
    "\n",
    "    return sentences, token_lists\n",
    "\n",
    "def save_vectors(method, vectors, data_file_name, vector_save_dir = vector_files_path):\n",
    "    \"\"\"\n",
    "    Save the vectors.\n",
    "    \"\"\"\n",
    "    # Define file names based on the input convention\n",
    "    file_base_name = f'{method}_{data_file_name}'\n",
    "\n",
    "    # Save vectors\n",
    "    with open(f\"{vector_save_dir}/{file_base_name}.file\", \"wb\") as f:\n",
    "        pickle.dump(vectors, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_vectors(method, data_file_name):\n",
    "    \"\"\"\n",
    "    Load the existing vectors.\n",
    "    \"\"\"\n",
    "    # Define file names based on the input convention\n",
    "    file_base_name = f'{method}_{data_file_name}'\n",
    "\n",
    "    # Load sentences file\n",
    "    with open(f\"{vector_files_path}/{file_base_name}.file\", \"rb\") as f:\n",
    "        vectors = pickle.load(f)\n",
    "\n",
    "    print(f\"Loaded vector file for {file_base_name}\")\n",
    "\n",
    "    return vectors\n",
    "\n",
    "def load_vectors_from_path(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        vectors = pickle.load(f)\n",
    "\n",
    "    print(f\"Loaded vector file from {path}\")\n",
    "\n",
    "    return vectors\n",
    "    \n",
    "def load_latest_topic_model(method, num_topics, gamma):\n",
    "    \"\"\"\n",
    "    Finds and loads the latest TopicModel file in the predefined model_files_path directory with the given method, num_topics, and gamma in the file name.\n",
    "\n",
    "    :param method: The method used in the topic model ('TFIDF', 'LDA', 'BERT', 'LDA_BERT').\n",
    "    :param num_topics: The number of topics used in the model (e.g., 10, 20).\n",
    "    :param gamma: The gamma value used in the model (e.g., 15).\n",
    "    :return: The latest TopicModel instance or None if no model is found.\n",
    "    \"\"\"\n",
    "    # Validate the method parameter\n",
    "    if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:\n",
    "        raise ValueError(\"Invalid method! Choose from 'TFIDF', 'LDA', 'BERT', 'LDA_BERT'.\")\n",
    "\n",
    "    # Validate num_topics is an integer\n",
    "    if not isinstance(num_topics, int):\n",
    "        raise ValueError(\"num_topics must be an integer.\")\n",
    "\n",
    "    # Validate gamma is a number (int or float)\n",
    "    if not isinstance(gamma, (int, float)):\n",
    "        raise ValueError(\"gamma must be an integer or float.\")\n",
    "\n",
    "    # Regular expression pattern to match the file name based on the method, num_topics, and gamma\n",
    "    pattern = rf'{method}_{data_file_name}_n_topics_{num_topics}_gamma_{gamma}_\\d{{4}}_\\d{{2}}_\\d{{2}}_\\d{{2}}_\\d{{2}}_\\d{{2}}\\.file'\n",
    "\n",
    "    directory_to_search = f'{model_files_path}/{method}'\n",
    "\n",
    "    # Get all files in the directory matching the pattern\n",
    "    model_files = [f for f in os.listdir(directory_to_search) if re.match(pattern, f)]\n",
    "\n",
    "    if model_files:\n",
    "        # Sort the files by the timestamp embedded in the filename\n",
    "        latest_model_file = max(model_files, key=lambda x: re.findall(r'\\d+', x))\n",
    "\n",
    "        # Construct the full path to the latest model file\n",
    "        model_path = os.path.join(directory_to_search, latest_model_file)\n",
    "\n",
    "        # Load the TopicModel object from the file\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            loaded_model = pickle.load(f)\n",
    "\n",
    "        if isinstance(loaded_model, TopicModel):\n",
    "            print(f\"Successfully loaded the latest TopicModel with method '{method}', {num_topics} topics, and gamma '{gamma}': {latest_model_file}\")\n",
    "            return loaded_model\n",
    "        else:\n",
    "            print(\"The loaded file is not an instance of TopicModel.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No model files found for method '{method}' with {num_topics} topics and gamma '{gamma}'.\")\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdef546",
   "metadata": {
    "papermill": {
     "duration": 0.036126,
     "end_time": "2024-11-23T06:07:56.127557",
     "exception": false,
     "start_time": "2024-11-23T06:07:56.091431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "https://chatgpt.com/share/672dbb3a-cf0c-800c-b1d5-0ff2d1a1c5fd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab5abd",
   "metadata": {
    "papermill": {
     "duration": 0.036544,
     "end_time": "2024-11-23T06:07:56.199760",
     "exception": false,
     "start_time": "2024-11-23T06:07:56.163216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TOPIC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07392026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:56.275498Z",
     "iopub.status.busy": "2024-11-23T06:07:56.274920Z",
     "iopub.status.idle": "2024-11-23T06:07:56.309763Z",
     "shell.execute_reply": "2024-11-23T06:07:56.309109Z"
    },
    "papermill": {
     "duration": 0.075108,
     "end_time": "2024-11-23T06:07:56.311395",
     "exception": false,
     "start_time": "2024-11-23T06:07:56.236287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define model object\n",
    "\n",
    "class TopicModel:\n",
    "    def __init__(self, \n",
    "            k=25,\n",
    "            alpha=0.01, \n",
    "            beta=0.5,\n",
    "            data_file_name=None,\n",
    "            token_lists=None,\n",
    "            sentences=None,\n",
    "            model_name=None,\n",
    "            use_n_grams=True,\n",
    "            tfidf_input_path=tfidf_input_path,\n",
    "            lda_vec_input_path=lda_input_path,\n",
    "            bert_input_path=song_embeddings_input_path\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the TopicModel class.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.model_name = model_name\n",
    "        self.model_name_print = f'{model_name} Topic model' if model_name else 'Default Topic model'\n",
    "        self.print_message(f'Initialized TopicModel with k={k}, alpha={alpha}, beta={beta}')\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        self.cluster_model = None\n",
    "        self.ldamodel = None\n",
    "        self.vec = {}\n",
    "        self.use_n_grams = use_n_grams\n",
    "        self.data_file_name = data_file_name  # Added data_file_name initialization\n",
    "        self.id = f'{data_file_name}_n_topics_{k}_{datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")}'\n",
    "        self.token_lists = token_lists\n",
    "        self.sentences = sentences\n",
    "        self.tfidf_corpus_in_path = f'{tfidf_input_path}/tfidf_corpus.file'\n",
    "        self.tfidf_dict_in_path = f'{tfidf_input_path}/tfidf_dict.file'\n",
    "        self.lda_model_in_path = f'{lda_input_path}/LDA_model_{data_file_name}.model'\n",
    "        self.lda_vec_in_path = f'{lda_vec_input_path}/LDA_{data_file_name}.file'\n",
    "        if os.path.exists(self.lda_model_in_path):\n",
    "            self.print_message(f'LDA model found at {self.lda_model_in_path}')\n",
    "        else:\n",
    "            self.print_message(f'LDA model not found at {self.lda_model_in_path}')\n",
    "        self.bert_vec_in_path = f'{bert_input_path}/Song_embeddings_preprocessed_sentences_{data_file_name}.file'\n",
    "        self.print_message(\"initialized\")\n",
    "\n",
    "    def print_message(self, message):\n",
    "        print(f\"{self.model_name_print}: {message}\")\n",
    "\n",
    "    def load_tfidf(self):\n",
    "        \"\"\"\n",
    "        Load TF-IDF vectors.\n",
    "        \"\"\"\n",
    "        self.print_message('Loading TF-IDF vectors ...')\n",
    "        if os.path.exists(self.tfidf_corpus_in_path) and os.path.exists(self.tfidf_dict_in_path):\n",
    "            with open(self.tfidf_corpus_in_path, 'rb') as f:\n",
    "                self.corpus = pickle.load(f)\n",
    "            self.dictionary = corpora.Dictionary.load(self.tfidf_dict_in_path.replace('pkl', 'dict'))\n",
    "            self.print_message('Loaded TF-IDF vectors')\n",
    "            return True\n",
    "        else:\n",
    "            self.print_message('TF-IDF vectors not found')\n",
    "            return False\n",
    "        \n",
    "    def save_tfidf(self):\n",
    "        \"\"\"\n",
    "        Save TF-IDF vectors.\n",
    "        \"\"\"\n",
    "        tfidf_name = f'{self.model_name}_tfidf' if self.model_name else 'tfidf'\n",
    "        save_vectors(tfidf_name, self.corpus, 'corpus')\n",
    "        save_vectors(tfidf_name, self.dictionary, 'dict')\n",
    "        self.tfidf_model.save(f'{vector_files_path}/{tfidf_name}_model.model')\n",
    "        self.dictionary.save(f'{vector_files_path}/{tfidf_name}_dict.dict')\n",
    "        self.print_message('Saved TF-IDF vectors')\n",
    "    \n",
    "    def vectorize_tfidf(self, no_below=5, no_above=0.5):\n",
    "        \"\"\"\n",
    "        Vectorize TF-IDF vectors.\n",
    "        \"\"\"\n",
    "        if not self.load_tfidf():\n",
    "            if self.use_n_grams:\n",
    "                self.print_message('Creating n-grams ...')\n",
    "                bigram_phrases = gensim.models.Phrases(self.token_lists, min_count=5, threshold=100)\n",
    "                trigram_phrases = gensim.models.Phrases(bigram_phrases[self.token_lists], threshold=100)\n",
    "                \n",
    "                bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "                trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "                bigram_name = f'{self.model_name}_bigram' if self.model_name else 'bigram'\n",
    "                trigram_name = f'{self.model_name}_trigram' if self.model_name else 'trigram'\n",
    "                n_grams_name = f'{self.model_name}_n_grams' if self.model_name else 'n_grams'\n",
    "\n",
    "                bigram.save(f'{vector_files_path}/{bigram_name}.phr')\n",
    "                trigram.save(f'{vector_files_path}/{trigram_name}.phr')\n",
    "\n",
    "                def make_bigrams(texts):\n",
    "                    return([bigram[doc] for doc in texts])\n",
    "\n",
    "                def make_trigrams(texts):\n",
    "                    return ([trigram[bigram[doc]] for doc in texts])\n",
    "\n",
    "                data_bigrams = make_bigrams(self.token_lists)\n",
    "                data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "                self.token_lists = data_bigrams_trigrams\n",
    "\n",
    "                save_vectors(n_grams_name, self.token_lists, 'n_grams_token_lists')\n",
    "\n",
    "                self.print_message('Created n-grams')\n",
    "                \n",
    "            self.print_message('Creating TF-IDF vectors ...')\n",
    "            dictionary = corpora.Dictionary(self.token_lists)\n",
    "            dictionary.filter_extremes(no_below=no_below, no_above=no_above)  \n",
    "            corpus = [dictionary.doc2bow(doc) for doc in self.token_lists]\n",
    "            self.tfidf_model = TfidfModel(corpus)\n",
    "            tfidf_corpus = [self.tfidf_model[doc] for doc in corpus]\n",
    "            self.corpus = tfidf_corpus\n",
    "            self.dictionary = dictionary\n",
    "            self.print_message('Created TF-IDF vectors')\n",
    "            self.save_tfidf()\n",
    "    \n",
    "    def load_lda(self, load_id2word = False):\n",
    "        \"\"\"\n",
    "        Load LDA model and vectors.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.lda_model_in_path) and os.path.exists(self.lda_vec_in_path):\n",
    "            # self.ldamodel = gensim.models.LdaModel.load(self.lda_model_in_path)\n",
    "            lda_model_file = self.lda_model_in_path.replace('.model', '.file')\n",
    "            with open(lda_model_file, 'rb') as f:\n",
    "                self.ldamodel = pickle.load(f)\n",
    "            with open(self.lda_vec_in_path, 'rb') as f:\n",
    "                self.vec['LDA'] = pickle.load(f)\n",
    "            if load_id2word:\n",
    "                self.dictionary = corpora.Dictionary.load(f'{self.lda_model_in_path}.id2word')\n",
    "                self.corpus = [self.dictionary.doc2bow(doc) for doc in self.token_lists]\n",
    "            self.print_message('Loaded LDA model and vectors')\n",
    "            return 1\n",
    "        elif os.path.exists(self.lda_model_in_path):\n",
    "            self.ldamodel = gensim.models.LdaModel.load(self.lda_model_in_path)\n",
    "            self.print_message('LDA model found, but vectors not found')\n",
    "            return 2\n",
    "        else:\n",
    "            self.print_message('LDA model and vectors not found')\n",
    "            return 3\n",
    "        \n",
    "    def get_lda_vectors_from_model(self):\n",
    "        self.print_message('Creating LDA vectors')\n",
    "        n_doc = len(self.corpus)\n",
    "        vec_lda = np.zeros((n_doc, self.k))\n",
    "        for i in range(n_doc):\n",
    "            # Get the distribution for the i-th document in corpus\n",
    "            for topic, prob in self.ldamodel.get_document_topics(self.corpus[i]):\n",
    "                vec_lda[i, topic] = prob\n",
    "        self.vec['LDA'] = vec_lda\n",
    "        self.print_message('Created LDA vectors')\n",
    "        self.save_lda()\n",
    "        self.evaluate_lda_model()\n",
    "\n",
    "    def get_lda_vectors(self):\n",
    "        \"\"\"\n",
    "        Get LDA vector representation based on the existing corpus.\n",
    "        \"\"\"\n",
    "        status = self.load_lda()\n",
    "        if status == 1:\n",
    "            return self.ldamodel, self.vec['LDA']\n",
    "        elif status == 2:\n",
    "            self.vectorize_tfidf()\n",
    "            self.get_lda_vectors_from_model()\n",
    "            return self.ldamodel, self.vec['LDA']\n",
    "        else:\n",
    "            self.vectorize_tfidf()\n",
    "            if not self.ldamodel:\n",
    "                self.print_message('Creating LDA model')\n",
    "                # self.ldamodel = gensim.models.LdaModel(corpus=self.corpus, num_topics=self.k, id2word=self.dictionary, passes=20)\n",
    "                self.ldamodel = gensim.models.LdaMulticore(corpus=self.corpus, num_topics=self.k, id2word=self.dictionary, workers=2, passes=20, random_state=42, alpha=self.alpha, eta=self.beta)\n",
    "                self.print_message('Created LDA model')\n",
    "            self.get_lda_vectors_from_model()\n",
    "            return self.ldamodel, self.vec['LDA']\n",
    "    \n",
    "    def save_lda(self):\n",
    "        \"\"\"\n",
    "        Save LDA model and vectors.\n",
    "        \"\"\"\n",
    "        lda_name = f'{self.model_name}_LDA' if self.model_name else 'LDA'\n",
    "        self.ldamodel.save(f'{vector_files_path}/{lda_name}_model_{self.data_file_name}.model')\n",
    "        save_vectors(f'{lda_name}_model', self.ldamodel, self.data_file_name)\n",
    "        save_vectors(lda_name, self.vec['LDA'], self.data_file_name)\n",
    "        self.print_message('Saved LDA model and vectors')\n",
    "\n",
    "    def evaluate_lda_model(self, coherence_type='c_v'):\n",
    "        \"\"\"\n",
    "        Evaluate the LDA model.\n",
    "        \"\"\"\n",
    "        self.print_message('Evaluating LDA model ...')\n",
    "        # self.load_tfidf()\n",
    "        # self.load_lda(load_id2word = False)\n",
    "        self.evaluate_perplexity()\n",
    "        self.evaluate_coherence(coherence_type)\n",
    "        # self.visualize_lda()\n",
    "\n",
    "    def evaluate_perplexity(self):\n",
    "        \"\"\"\n",
    "        Evaluate the perplexity of the LDA model.\n",
    "        \"\"\"\n",
    "        if not self.ldamodel or not self.corpus:\n",
    "            self.print_message(\"LDA model or corpus is not available for perplexity evaluation.\")\n",
    "            return None\n",
    "        \n",
    "        perplexity = self.ldamodel.log_perplexity(self.corpus)\n",
    "        self.print_message(f\"Perplexity: {perplexity}\")\n",
    "        return perplexity\n",
    "\n",
    "    def evaluate_coherence(self, coherence_type='c_v'):\n",
    "        \"\"\"\n",
    "        Evaluate the coherence score of the LDA model.\n",
    "        :param coherence_type: The type of coherence measure to use (default is 'c_v').\n",
    "        \"\"\"\n",
    "        if not self.ldamodel or not self.token_lists or not self.dictionary:\n",
    "            self.print_message(\"LDA model, token lists, or dictionary is not available for coherence evaluation.\")\n",
    "            return None\n",
    "        \n",
    "        coherence_model = CoherenceModel(model=self.ldamodel, \n",
    "                                         texts=self.token_lists, \n",
    "                                         dictionary=self.dictionary, \n",
    "                                         coherence=coherence_type)\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        self.print_message(f\"Coherence Score ({coherence_type}): {coherence_score}\")\n",
    "        return coherence_score\n",
    "\n",
    "    def visualize_lda(self):\n",
    "        \"\"\"\n",
    "        Visualizes the LDA model using pyLDAvis.\n",
    "        \"\"\"\n",
    "        if not self.ldamodel or not self.dictionary or not self.corpus:\n",
    "            self.print_message(\"LDA model, dictionary, or corpus is not available for visualization.\")\n",
    "            return\n",
    "        \n",
    "        # Prepare the pyLDAvis visualization\n",
    "        vis = pyLDAvis.gensim_models.prepare(self.ldamodel, self.corpus, self.dictionary)\n",
    "        \n",
    "        # Display the visualization (works well in Jupyter notebooks)\n",
    "        # pyLDAvis.display(vis)\n",
    "        \n",
    "        # Save the visualization to an HTML file\n",
    "        lda_name = f'{self.model_name}_LDA' if self.model_name else 'LDA'\n",
    "        vis_file = f'{vector_files_path}/{lda_name}_{self.data_file_name}_visualization.html'\n",
    "        vis.save_html(vis_file)\n",
    "        self.print_message(f\"Saved pyLDAvis visualization to {vis_file}\")\n",
    "\n",
    "    # Method for hyperparameter tuning of LDA model\n",
    "    def tune_lda_hyperparameters(self, k_values=[5, 10, 15], alpha_values=[0.01, 0.1, 0.5], beta_values=[0.01, 0.1, 0.5], passes=20):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter tuning for LDA model.\n",
    "        \n",
    "        :param k_values: List of number of topics to try.\n",
    "        :param alpha_values: List of alpha values to try.\n",
    "        :param beta_values: List of beta values to try.\n",
    "        :param passes: Number of passes for the LDA model.\n",
    "        \"\"\"\n",
    "        self.vectorize_tfidf()\n",
    "        best_coherence = -np.inf\n",
    "        best_params = {'k': None, 'alpha': None, 'beta': None}\n",
    "        best_lda_model = None\n",
    "\n",
    "        configurations = [\n",
    "            (20, 0.01, 0.01),\n",
    "            (20, 0.01, 0.5),\n",
    "            (20, 0.1, 0.5),\n",
    "            (20, 0.5, 0.5),\n",
    "            (25, 0.01, 0.01),\n",
    "            (25, 0.01, 0.5),\n",
    "            (25, 0.1, 0.5),\n",
    "            (25, 0.5, 0.5),\n",
    "            (15, 0.01, 0.5)\n",
    "        ]\n",
    "        \n",
    "        for k, alpha, beta in configurations:\n",
    "            self.print_message(f\"Tuning with k={k}, alpha={alpha}, beta={beta}\")\n",
    "            \n",
    "            # Train LDA model with current hyperparameters\n",
    "            self.ldamodel = gensim.models.LdaMulticore(\n",
    "                corpus=self.corpus,\n",
    "                num_topics=k,\n",
    "                id2word=self.dictionary,\n",
    "                alpha=alpha,\n",
    "                eta=beta,\n",
    "                passes=passes,\n",
    "                workers=2,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            # Evaluate the coherence score\n",
    "            coherence_score = self.evaluate_coherence(coherence_type='c_v')\n",
    "\n",
    "            # Update the best model if needed\n",
    "            if coherence_score > best_coherence:\n",
    "                best_coherence = coherence_score\n",
    "                best_params = {'k': k, 'alpha': alpha, 'beta': beta}\n",
    "                best_lda_model = self.ldamodel\n",
    "\n",
    "            self.print_message(f\"Best Coherence Score: {best_coherence}\")\n",
    "            self.print_message(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "        # Loop through all combinations of hyperparameters\n",
    "        # for k in k_values:\n",
    "        #     for alpha in alpha_values:\n",
    "        #         for beta in beta_values:\n",
    "        #             self.print_message(f\"Tuning with k={k}, alpha={alpha}, beta={beta}\")\n",
    "                    \n",
    "        #             # Train LDA model with current hyperparameters\n",
    "        #             self.ldamodel = gensim.models.LdaMulticore(\n",
    "        #                 corpus=self.corpus,\n",
    "        #                 num_topics=k,\n",
    "        #                 id2word=self.dictionary,\n",
    "        #                 alpha=alpha,\n",
    "        #                 eta=beta,\n",
    "        #                 passes=passes,\n",
    "        #                 workers=2,\n",
    "        #                 random_state=42\n",
    "        #             )\n",
    "\n",
    "        #             # Evaluate the coherence score\n",
    "        #             coherence_score = self.evaluate_coherence(coherence_type='c_v')\n",
    "\n",
    "        #             # Update the best model if needed\n",
    "        #             if coherence_score > best_coherence:\n",
    "        #                 best_coherence = coherence_score\n",
    "        #                 best_params = {'k': k, 'alpha': alpha, 'beta': beta}\n",
    "        #                 best_lda_model = self.ldamodel\n",
    "\n",
    "        #             self.print_message(f\"Best Coherence Score: {best_coherence}\")\n",
    "        #             self.print_message(f\"Best Hyperparameters: {best_params}\")\n",
    "        \n",
    "        # Save the best LDA model\n",
    "        if best_lda_model:\n",
    "            self.ldamodel = best_lda_model\n",
    "            self.get_lda_vectors_from_model()\n",
    "            self.save_lda()\n",
    "            self.print_message(f\"Saved best LDA model with {best_params}\")\n",
    "        return best_params, best_coherence\n",
    "        \n",
    "    def load_bert(self):\n",
    "        \"\"\"\n",
    "        Load BERT vectors.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.bert_vec_in_path):\n",
    "            with open(self.bert_vec_in_path, 'rb') as f:\n",
    "                self.vec['BERT'] = pickle.load(f)\n",
    "            self.print_message('Loaded Song embeddings')\n",
    "            return True\n",
    "        else:\n",
    "            self.print_message('Song embeddings not found')\n",
    "            return False\n",
    "    \n",
    "    def get_bert_vectors(self):\n",
    "        \"\"\"\n",
    "        Get BERT vector representations.\n",
    "        \"\"\"\n",
    "        if self.load_bert():\n",
    "            return self.vec['BERT']\n",
    "        else:\n",
    "            self.print_message('Creating Song embeddings ...')\n",
    "            # model = SentenceTransformer('bert-base-nli-max-tokens')\n",
    "            model = SentenceTransformer('all-mpnet-base-v2')\n",
    "            vec = np.array(model.encode(self.sentences, show_progress_bar=True))\n",
    "            self.vec['BERT'] = vec\n",
    "            self.print_message('Created Song embeddings')\n",
    "            self.save_bert()\n",
    "            return vec\n",
    "    \n",
    "    def save_bert(self):\n",
    "        \"\"\"\n",
    "        Save BERT vectors.\n",
    "        \"\"\"\n",
    "        bert_name = f'{self.model_name}_Song_embeddings_preprocessed_sentences' if self.model_name else 'Song_embeddings_preprocessed_sentences'\n",
    "        save_vectors(bert_name, self.vec['BERT'], self.data_file_name)\n",
    "        self.print_message('Saved Song embeddings')\n",
    "\n",
    "    def get_lda_bert_vectors(self):\n",
    "        \"\"\"\n",
    "        Get LDA and BERT vector representations.\n",
    "        \"\"\"\n",
    "        self.get_lda_vectors()\n",
    "        self.get_bert_vectors()\n",
    "        return self.vec['LDA'], self.vec['BERT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc1ed02",
   "metadata": {
    "papermill": {
     "duration": 0.038849,
     "end_time": "2024-11-23T06:07:56.388067",
     "exception": false,
     "start_time": "2024-11-23T06:07:56.349218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d0fea87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:56.467818Z",
     "iopub.status.busy": "2024-11-23T06:07:56.467487Z",
     "iopub.status.idle": "2024-11-23T06:07:56.502823Z",
     "shell.execute_reply": "2024-11-23T06:07:56.502105Z"
    },
    "papermill": {
     "duration": 0.076948,
     "end_time": "2024-11-23T06:07:56.504453",
     "exception": false,
     "start_time": "2024-11-23T06:07:56.427505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output + x  # Residual connection\n",
    "\n",
    "\n",
    "class LDAEncoder(nn.Module):\n",
    "    def __init__(self, lda_dim, latent_dim):\n",
    "        super(LDAEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(lda_dim, 128),\n",
    "            nn.GELU(),\n",
    "            SelfAttention(128),\n",
    "            nn.Linear(128, latent_dim),\n",
    "            nn.LayerNorm(latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class MPNetEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, latent_dim):\n",
    "        super(MPNetEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(1, 1, kernel_size=3, padding=1),\n",
    "            GatedLinearUnit(),\n",
    "            SelfAttention(256),\n",
    "            nn.Linear(256, latent_dim),\n",
    "            nn.LayerNorm(latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for Conv1d\n",
    "        return self.encoder(x).squeeze(1)\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=dim, num_heads=4, batch_first=True)\n",
    "\n",
    "    def forward(self, z_lda, z_mpnet):\n",
    "        # Cross-attention between LDA and MPNet latent representations\n",
    "        attn_output, _ = self.attention(z_lda, z_mpnet, z_mpnet)\n",
    "        return attn_output + z_lda  # Residual connection\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, lda_dim, embedding_dim, latent_dim=64):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Separate encoders for LDA and MPNet\n",
    "        self.lda_encoder = LDAEncoder(lda_dim, latent_dim)\n",
    "        self.mpnet_encoder = MPNetEncoder(embedding_dim, latent_dim)\n",
    "        \n",
    "        # Cross-attention to combine LDA and MPNet latent spaces\n",
    "        self.cross_attention = CrossAttention(latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2 * latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, lda_dim + embedding_dim)\n",
    "        )\n",
    "\n",
    "        # Variational layers (optional for VAE)\n",
    "        self.fc_mu = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, lda_input, mpnet_input):\n",
    "        # LDA encoding\n",
    "        lda_z = self.lda_encoder(lda_input)\n",
    "        \n",
    "        # MPNet encoding\n",
    "        mpnet_z = self.mpnet_encoder(mpnet_input)\n",
    "        \n",
    "        # Cross-attention between LDA and MPNet latent representations\n",
    "        combined_z = self.cross_attention(lda_z, mpnet_z)\n",
    "        \n",
    "        # Reparameterization for variational autoencoder (optional)\n",
    "        mu, logvar = self.fc_mu(combined_z), self.fc_logvar(combined_z)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # Decoder\n",
    "        concatenated_z = torch.cat([z, mpnet_z], dim=1)\n",
    "        reconstructed = self.decoder(concatenated_z)\n",
    "        \n",
    "        return reconstructed, z, mu, logvar\n",
    "\n",
    "\n",
    "class GatedLinearUnit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GatedLinearUnit, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class AutoencoderHandler():\n",
    "    def __init__(self, lda_data, embeddings, autoencoder_name='autoencoder', scaled=False, autoencoder_in_path = autoencoder_input_path, latent_dim=64, num_epochs=30, batch_size=64, checkpoint_dir=\"checkpoints\", log_interval=100, data_file_name='data_file_name'):\n",
    "        self.lda_data = lda_data\n",
    "        self.embeddings = embeddings\n",
    "        self.autoencoder_name = autoencoder_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.scaled = scaled\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.log_interval = log_interval\n",
    "        self.data_file_name = data_file_name\n",
    "        self.autoencoder_in_path = autoencoder_in_path\n",
    "\n",
    "        self.lda_scaler = None\n",
    "        self.embedding_scaler = None\n",
    "        self.load_scalers()\n",
    "        self.get_scaled_data()\n",
    "        \n",
    "        self.lda_data = torch.tensor(self.scaled_lda_data, dtype=torch.float32).to(self.device)\n",
    "        self.embeddings = torch.tensor(self.scaled_embeddings, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        self.dataset = TensorDataset(self.lda_data, self.embeddings)\n",
    "        self.dataloader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        self.model = Autoencoder(lda_dim=self.scaled_lda_data.shape[1], embedding_dim=self.scaled_embeddings.shape[1], latent_dim=self.latent_dim).to(self.device)\n",
    "\n",
    "        self.load_model()\n",
    "        \n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=1e-4)\n",
    "        self.losses = []\n",
    "        self.print_message(f'{self.autoencoder_name} initialized')\n",
    "\n",
    "    def print_message(self, message):\n",
    "        print(f'{self.autoencoder_name}: {message}')\n",
    "\n",
    "    def load_scalers(self):\n",
    "        lda_scaler_path = f'{self.autoencoder_in_path}/autoencoder_train_lda_scaler_{self.data_file_name}.file'\n",
    "        embedding_scaler_path = f'{self.autoencoder_in_path}/autoencoder_train_embedding_scaler_{self.data_file_name}.file'\n",
    "        if os.path.exists(lda_scaler_path) and os.path.exists(embedding_scaler_path):\n",
    "            self.lda_scaler = load_vectors_from_path(lda_scaler_path)\n",
    "            self.embedding_scaler = load_vectors_from_path(embedding_scaler_path)\n",
    "            self.print_message(f'Scalers loaded from {lda_scaler_path} and {embedding_scaler_path}')\n",
    "\n",
    "    def load_model(self):\n",
    "        autoencoder_model_path = f'{self.autoencoder_in_path}/best_model.pth'\n",
    "        if os.path.exists(autoencoder_model_path):\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            self.model.load_state_dict(torch.load(autoencoder_model_path, map_location=device))\n",
    "            self.print_message(f'Model loaded from {autoencoder_model_path}')\n",
    "    \n",
    "    def generate_validation_representations(self):\n",
    "        \"\"\"\n",
    "        Generate latent representations for the validation dataset using the trained autoencoder.\n",
    "        \n",
    "        Returns:\n",
    "            latent_representations (numpy.ndarray): Latent representations generated for the validation dataset.\n",
    "        \"\"\"\n",
    "        self.print_message('Generating validation representations')\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "        # Collect latent representations\n",
    "        latent_representations = []\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculations\n",
    "            for lda_input, song_embedding in self.dataloader:\n",
    "                _, latent_z, _, _ = self.model(lda_input, song_embedding)\n",
    "                latent_representations.append(latent_z.cpu().numpy())\n",
    "        \n",
    "        latent_representations = np.concatenate(latent_representations, axis=0)\n",
    "        \n",
    "        save_vectors(f'{self.autoencoder_name}_latent_representations', latent_representations, self.data_file_name)\n",
    "        self.print_message(f'Validation representations saved to {self.autoencoder_name}_latent_representations_{self.data_file_name}.file')\n",
    "\n",
    "        return latent_representations\n",
    "\n",
    "    def get_scaled_data(self):\n",
    "        if not self.scaled:\n",
    "            save_scalers = self.lda_scaler is None or self.embedding_scaler is None\n",
    "            self.lda_scaler = StandardScaler() if self.lda_scaler is None else self.lda_scaler\n",
    "            self.embedding_scaler = StandardScaler() if self.embedding_scaler is None else self.embedding_scaler\n",
    "            self.scaled_lda_data = self.lda_scaler.fit_transform(self.lda_data)\n",
    "            self.scaled_embeddings = self.embedding_scaler.fit_transform(self.embeddings)\n",
    "            if save_scalers:\n",
    "                save_vectors(f'{self.autoencoder_name}_lda_scaler', self.lda_scaler, self.data_file_name)\n",
    "                save_vectors(f'{self.autoencoder_name}_embedding_scaler', self.embedding_scaler, self.data_file_name)\n",
    "                self.print_message(f'Scalers saved to {self.autoencoder_name}_lda_scaler_{self.data_file_name}.file and {self.autoencoder_name}_embedding_scaler_{self.data_file_name}.file')\n",
    "            save_vectors(f'{self.autoencoder_name}_scaled_lda', self.scaled_lda_data, self.data_file_name)\n",
    "            save_vectors(f'{self.autoencoder_name}_scaled_embeddings', self.scaled_embeddings, self.data_file_name)\n",
    "        else:\n",
    "            self.scaled_lda_data = self.lda_data\n",
    "            self.scaled_embeddings = self.embeddings\n",
    "            self.print_message('Scaled data already exists')\n",
    "\n",
    "    def contrastive_loss(self, z_anchor, z_positive, z_negative, margin=1.0):\n",
    "        pos_dist = nn.functional.cosine_similarity(z_anchor, z_positive)\n",
    "        neg_dist = nn.functional.cosine_similarity(z_anchor, z_negative)\n",
    "        return torch.mean(torch.relu(margin + neg_dist - pos_dist))\n",
    "\n",
    "    def combined_loss(self, reconstructed, original, z_anchor, z_positive, z_negative, mu, logvar, epoch, max_epochs, beta=0.01, contrastive_weight=0.1):\n",
    "        reconstruction_loss = nn.MSELoss()(reconstructed, original)\n",
    "        kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        contrast_loss = self.contrastive_loss(z_anchor, z_positive, z_negative)\n",
    "        dynamic_contrastive_weight = contrastive_weight * (epoch / max_epochs)\n",
    "        \n",
    "        return reconstruction_loss + beta * kl_divergence + dynamic_contrastive_weight * contrast_loss\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        scaler = GradScaler()\n",
    "        best_loss = float(\"inf\")\n",
    "        no_improvement_epochs = 0\n",
    "        latent_reps = []\n",
    "        patience = 3\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for batch_idx, (lda_input, song_embedding) in enumerate(self.dataloader):\n",
    "                lda_input, song_embedding = lda_input.to(self.device), song_embedding.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass with autocast for mixed precision\n",
    "                with autocast():\n",
    "                    reconstructed, z_anchor, mu, logvar = self.model(lda_input, song_embedding)\n",
    "                    latent_reps.append(z_anchor.detach().cpu().numpy())\n",
    "\n",
    "                    # Generate positive and negative pairs\n",
    "                    positive_idx = (batch_idx + 1) % len(self.dataloader)\n",
    "                    negative_idx = (batch_idx + 2) % len(self.dataloader)\n",
    "                    lda_pos, emb_pos = self.dataloader.dataset[positive_idx]\n",
    "                    lda_neg, emb_neg = self.dataloader.dataset[negative_idx]\n",
    "                    lda_pos, emb_pos = lda_pos.to(self.device), emb_pos.to(self.device)\n",
    "                    lda_neg, emb_neg = lda_neg.to(self.device), emb_neg.to(self.device)\n",
    "\n",
    "                    _, z_positive, _, _ = self.model(lda_pos.unsqueeze(0), emb_pos.unsqueeze(0))\n",
    "                    _, z_negative, _, _ = self.model(lda_neg.unsqueeze(0), emb_neg.unsqueeze(0))\n",
    "\n",
    "                    # Calculate combined loss\n",
    "                    loss = self.combined_loss(\n",
    "                        reconstructed, torch.cat((lda_input, song_embedding), dim=1),\n",
    "                        z_anchor, z_positive, z_negative, mu, logvar, epoch, self.num_epochs\n",
    "                    )\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(self.optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if batch_idx % self.log_interval == 0:\n",
    "                    self.print_message(f\"Epoch [{epoch + 1}/{self.num_epochs}], Batch [{batch_idx}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "            avg_epoch_loss = epoch_loss / len(self.dataloader)\n",
    "            self.print_message(f\"Epoch [{epoch + 1}/{self.num_epochs}], Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "            # Check for early stopping condition\n",
    "            if avg_epoch_loss < best_loss:\n",
    "                best_loss = avg_epoch_loss\n",
    "                no_improvement_epochs = 0\n",
    "                torch.save(self.model.state_dict(), f\"{self.checkpoint_dir}/best_model.pth\")\n",
    "                self.print_message(f\"Checkpoint saved with Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "            else:\n",
    "                no_improvement_epochs += 1\n",
    "                self.print_message(f\"No improvement in validation loss for {no_improvement_epochs} epoch(s).\")\n",
    "            \n",
    "            # Stop training if no improvement for 'patience' epochs\n",
    "            if no_improvement_epochs >= patience:\n",
    "                self.print_message(f\"Early stopping triggered. Stopping training after epoch {epoch + 1}.\")\n",
    "                break\n",
    "\n",
    "            self.losses.append(avg_epoch_loss)\n",
    "\n",
    "        latent_representations = np.concatenate(latent_reps)\n",
    "        save_vectors(f'{self.autoencoder_name}_latent_representations', latent_representations, self.data_file_name)\n",
    "\n",
    "        # summary(self.model, input_size=(self.scaled_lda_data.shape[1], self.scaled_embeddings.shape[1]), batch_size=self.batch_size)\n",
    "\n",
    "        self.plot_loss_graph()\n",
    "\n",
    "    def plot_loss_graph(self):\n",
    "        plt.plot(range(1, len(self.losses) + 1), self.losses)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Average Loss')\n",
    "        plt.title('Training Loss over Epochs')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'{self.checkpoint_dir}/{self.autoencoder_name}_loss_graph.png')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e8ac35f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:56.580430Z",
     "iopub.status.busy": "2024-11-23T06:07:56.580171Z",
     "iopub.status.idle": "2024-11-23T06:07:56.602341Z",
     "shell.execute_reply": "2024-11-23T06:07:56.601752Z"
    },
    "papermill": {
     "duration": 0.062345,
     "end_time": "2024-11-23T06:07:56.604083",
     "exception": false,
     "start_time": "2024-11-23T06:07:56.541738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdvancedSongRecommender:\n",
    "    def __init__(self, latent_representations: np.ndarray, reduced_dim_latent_representations: np.ndarray, song_lyrics: List[str], top_k: int = 50, cache_dir_in=song_recommender_cache_input_path, cache_dir_out=song_recommender_cache_out_path):\n",
    "        \"\"\"\n",
    "        Initialize the recommender with latent representations and lyrics data.\n",
    "\n",
    "        Parameters:\n",
    "        - latent_representations: Latent representations of songs.\n",
    "        - song_lyrics: List of lyrics corresponding to each song.\n",
    "        - top_k: Number of top recommendations to return.\n",
    "        - cache_dir_in: Directory where cache files will be read from.\n",
    "        - cache_dir_out: Directory where cache files will be saved.\n",
    "        \"\"\"\n",
    "        self.latent_representations = latent_representations\n",
    "        self.reduced_dim_latent_representations = reduced_dim_latent_representations\n",
    "        self.song_lyrics = song_lyrics\n",
    "        self.top_k = top_k\n",
    "        self.index_file_in = f\"{cache_dir_in}/faiss_index.idx\"\n",
    "        self.cluster_file_in = f\"{cache_dir_in}/cluster_labels.npy\"\n",
    "        self.index_file_out = f\"{cache_dir_out}/faiss_index.idx\"\n",
    "        self.cluster_file_out = f\"{cache_dir_out}/cluster_labels.npy\"\n",
    "        self.theme_cache_file_in = f\"{cache_dir_in}/cluster_themes.pkl\"\n",
    "        self.theme_cache_file_out = f\"{cache_dir_out}/cluster_themes.pkl\"\n",
    "\n",
    "        print(\"Building FAISS index for global candidate retrieval\")\n",
    "        # Step 1: FAISS Index for Global Candidate Retrieval\n",
    "        self.faiss_index = self._build_faiss_index()\n",
    "        \n",
    "        print(\"Building HDBSCAN clusters\")\n",
    "        # Step 2: HDBSCAN Global Clustering (precomputed for theme clusters)\n",
    "        self.cluster_labels = self._build_hdbscan_clusters()\n",
    "        \n",
    "        print(\"Storing songs in each cluster for fast retrieval\")\n",
    "        # Store songs in each cluster for fast retrieval\n",
    "        self.cluster_members = self._create_cluster_members()\n",
    "\n",
    "        # print(\"Building Gensim LDA model for theme extraction\")\n",
    "        # # Step 3: Build Gensim LDA model for theme extraction\n",
    "        # self.lda_model, self.dictionary = self._train_gensim_lda_model()\n",
    "\n",
    "        # print(\"Precomputing and caching cluster themes\")\n",
    "        # # Step 4: Precompute and cache cluster themes during initialization\n",
    "        # self.theme_cache = self._precompute_and_cache_themes()\n",
    "\n",
    "    def _build_faiss_index(self):\n",
    "        \"\"\"Build or load a FAISS index with optimized parameters for a large dataset.\"\"\"\n",
    "        index = self._load_faiss_index()\n",
    "        if not index:  # Load existing index if available\n",
    "            d = self.latent_representations.shape[1]\n",
    "            nlist = 4096  # Number of Voronoi cells\n",
    "            m = 16  # Number of sub-quantizers for PQ\n",
    "            index = faiss.IndexIVFPQ(faiss.IndexFlatIP(d), d, nlist, m, 8)  # 8 bits per sub-vector\n",
    "            index.train(self.latent_representations)\n",
    "            index.add(self.latent_representations)\n",
    "            index.nprobe = 20  # Number of clusters to search during querying\n",
    "            faiss.write_index(index, self.index_file_out)  # Save for future use\n",
    "            print(\"FAISS index built and saved.\")\n",
    "        return index\n",
    "\n",
    "    def _load_faiss_index(self):\n",
    "        \"\"\"Load the FAISS index if it exists.\"\"\"\n",
    "        try:\n",
    "            faiss_index = faiss.read_index(self.index_file_in)\n",
    "            print(\"FAISS index loaded from disk.\")\n",
    "            return faiss_index\n",
    "        except:\n",
    "            print(\"No pre-existing FAISS index found; building a new one.\")\n",
    "            return None\n",
    "\n",
    "    def _build_hdbscan_clusters(self):\n",
    "        \"\"\"Build or load HDBSCAN clustering model.\"\"\"\n",
    "        if not self._load_clusters():\n",
    "            hdbscan_clusterer = HDBSCAN(min_cluster_size=1000, min_samples=100, metric='euclidean', verbose=True)\n",
    "            cluster_labels = hdbscan_clusterer.fit_predict(self.reduced_dim_latent_representations)\n",
    "            np.save(self.cluster_file_out, cluster_labels)  # Save cluster labels for future use\n",
    "            print(\"HDBSCAN clustering done and saved.\")\n",
    "        return cluster_labels\n",
    "\n",
    "    def _load_clusters(self):\n",
    "        \"\"\"Load precomputed cluster labels if available.\"\"\"\n",
    "        try:\n",
    "            self.cluster_labels = np.load(self.cluster_file_in)\n",
    "            print(\"Cluster labels loaded from disk.\")\n",
    "            return True\n",
    "        except:\n",
    "            print(\"No pre-existing cluster labels found; building clusters.\")\n",
    "            return False\n",
    "\n",
    "    def _create_cluster_members(self) -> Dict[int, List[int]]:\n",
    "        \"\"\"Organize songs into clusters for fast cluster-based retrieval.\"\"\"\n",
    "        cluster_dict = {}\n",
    "        for idx, label in enumerate(self.cluster_labels):\n",
    "            if label != -1:  # Ignore noise points\n",
    "                cluster_dict.setdefault(label, []).append(idx)\n",
    "        return cluster_dict\n",
    "\n",
    "    def _train_gensim_lda_model(self) -> tuple:\n",
    "        \"\"\"Train a Gensim LDA model to extract topics/themes from song lyrics.\"\"\"\n",
    "        # Tokenize the lyrics (simple whitespace tokenizer)\n",
    "        texts = [song.split() for song in self.song_lyrics]\n",
    "        \n",
    "        # Create a dictionary and corpus for Gensim LDA model\n",
    "        dictionary = Dictionary(texts)\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "        # Train the LDA model with 10 topics\n",
    "        lda_model = LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)\n",
    "        print(\"Gensim LDA model trained for theme extraction.\")\n",
    "        return lda_model, dictionary\n",
    "\n",
    "    def _precompute_and_cache_themes(self) -> Dict[int, str]:\n",
    "        \"\"\"Precompute and cache the themes for all clusters during initialization.\"\"\"\n",
    "        theme_cache = self._load_theme_cache()\n",
    "        if theme_cache:\n",
    "            return theme_cache\n",
    "        else:\n",
    "            theme_cache = {}\n",
    "            for cluster_id in set(self.cluster_labels):\n",
    "                if cluster_id != -1:  # Exclude noise points\n",
    "                    theme_cache[cluster_id] = self._get_cluster_theme(cluster_id)\n",
    "            # Save the cached themes to disk for later use\n",
    "            self._save_theme_cache(theme_cache)\n",
    "            print(\"Cluster themes precomputed and cached.\")\n",
    "            return theme_cache\n",
    "    \n",
    "    def _load_theme_cache(self):\n",
    "        \"\"\"Load precomputed cluster themes if available.\"\"\"\n",
    "        if os.path.exists(self.theme_cache_file_in):\n",
    "            with open(self.theme_cache_file_in, 'rb') as f:\n",
    "                theme_cache = pickle.load(f)\n",
    "            print(\"Cluster themes loaded from disk.\")\n",
    "            return theme_cache\n",
    "        else:\n",
    "            print(\"No pre-existing cluster themes found; building clusters.\")\n",
    "            return None\n",
    "\n",
    "    def _get_cluster_theme(self, cluster_id: int) -> str:\n",
    "        \"\"\"Generate a theme summary for a given cluster using Gensim LDA model.\"\"\"\n",
    "        cluster_songs = self.cluster_members.get(cluster_id, [])\n",
    "        lyrics_for_cluster = [self.song_lyrics[song_id] for song_id in cluster_songs]\n",
    "\n",
    "        # Tokenize the songs' lyrics for LDA analysis\n",
    "        texts_for_cluster = [song.split() for song in lyrics_for_cluster]\n",
    "        corpus_for_cluster = [self.dictionary.doc2bow(text) for text in texts_for_cluster]\n",
    "\n",
    "        # Get the topics distribution for the cluster's songs\n",
    "        topic_distribution = self.lda_model[corpus_for_cluster]\n",
    "\n",
    "        # Collect the most frequent terms for the top topics\n",
    "        top_topics = [max(doc, key=lambda x: x[1])[0] for doc in topic_distribution]\n",
    "        topic_keywords = []\n",
    "        for topic_idx in top_topics:\n",
    "            keywords = [word for word, _ in self.lda_model.show_topic(topic_idx, topn=5)]\n",
    "            topic_keywords.append(\", \".join(keywords))\n",
    "\n",
    "        # Summarize the theme of the cluster\n",
    "        theme_summary = \" | \".join(topic_keywords)\n",
    "        return theme_summary\n",
    "\n",
    "    def _save_theme_cache(self, theme_cache: Dict[int, str]):\n",
    "        \"\"\"Save the theme cache to disk.\"\"\"\n",
    "        with open(self.theme_cache_file_out, 'wb') as f:\n",
    "            pickle.dump(theme_cache, f)\n",
    "        print(\"Theme cache saved.\")\n",
    "\n",
    "    def recommend(self, song_id: int, novelty: float = 0.5) -> List[int]:\n",
    "        \"\"\"\n",
    "        Generate recommendations with adjustable novelty using FAISS and cluster-based filtering.\n",
    "\n",
    "        Parameters:\n",
    "        - song_id: ID of the song to base recommendations on.\n",
    "        - novelty: Control the diversity (0 = similar songs, 1 = diverse songs).\n",
    "\n",
    "        Returns:\n",
    "        - List of recommended song IDs with theme explanation.\n",
    "        \"\"\"\n",
    "        print(\"Starting recommendations for song_id %d\", song_id)\n",
    "\n",
    "        # Step 1: Retrieve top-k global candidates from FAISS\n",
    "        _, top_candidates = self.faiss_index.search(self.latent_representations[song_id:song_id+1], self.top_k)\n",
    "        # candidate_reps = self.latent_representations[top_candidates.flatten()]\n",
    "        \n",
    "        # Step 2: Cluster-Based Filtering\n",
    "        target_cluster = self.cluster_labels[song_id]\n",
    "        main_cluster_songs = self.cluster_members.get(target_cluster, [])\n",
    "        \n",
    "        # Adjust clusters based on novelty\n",
    "        if novelty < 0.5:\n",
    "            # Low novelty: Prioritize songs within main cluster\n",
    "            filtered_candidates = [song for song in top_candidates.flatten() if song in main_cluster_songs]\n",
    "        else:\n",
    "            # High novelty: Include songs from neighboring clusters\n",
    "            nearby_clusters = self._get_nearby_clusters(target_cluster, novelty)\n",
    "            filtered_candidates = [song for song in top_candidates.flatten() if song in nearby_clusters]\n",
    "\n",
    "        # Step 3: Intra-Cluster Ranking (Cosine similarity)\n",
    "        scores = cosine_similarity(\n",
    "            self.reduced_dim_latent_representations[song_id:song_id+1],\n",
    "            self.reduced_dim_latent_representations[filtered_candidates]\n",
    "        ).flatten()\n",
    "        ranked_candidates = [filtered_candidates[i] for i in np.argsort(scores)]\n",
    "\n",
    "        # Step 4: Get Themes and Explanation\n",
    "        theme_explanations = []\n",
    "        for song in ranked_candidates:\n",
    "            theme = self._get_cluster_theme(self.cluster_labels[song])\n",
    "            theme_explanations.append(theme)\n",
    "\n",
    "        return ranked_candidates[:self.top_k], theme_explanations\n",
    "\n",
    "    def _get_nearby_clusters(self, target_cluster: int, novelty_factor: float = 0.5) -> List[int]:\n",
    "        \"\"\"Get nearby clusters based on centroids' distances (simulate diverse recommendations).\"\"\"\n",
    "        cluster_distances = {\n",
    "            label: np.linalg.norm(\n",
    "                np.mean(self.reduced_dim_latent_representations[self.cluster_members[label]], axis=0) -\n",
    "                np.mean(self.reduced_dim_latent_representations[self.cluster_members[target_cluster]], axis=0)\n",
    "            )\n",
    "            for label in self.cluster_members\n",
    "        }\n",
    "        sorted_clusters = sorted(cluster_distances.items(), key=lambda x: x[1])\n",
    "        \n",
    "        # Adjust number of clusters included based on novelty\n",
    "        max_clusters = int(len(sorted_clusters) * novelty_factor)\n",
    "        return [label for label, _ in sorted_clusters[:max_clusters]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d9bee",
   "metadata": {
    "papermill": {
     "duration": 0.036459,
     "end_time": "2024-11-23T06:07:56.676571",
     "exception": false,
     "start_time": "2024-11-23T06:07:56.640112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66a6e3d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:56.751082Z",
     "iopub.status.busy": "2024-11-23T06:07:56.750051Z",
     "iopub.status.idle": "2024-11-23T06:07:56.754134Z",
     "shell.execute_reply": "2024-11-23T06:07:56.753463Z"
    },
    "papermill": {
     "duration": 0.043059,
     "end_time": "2024-11-23T06:07:56.755920",
     "exception": false,
     "start_time": "2024-11-23T06:07:56.712861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if not load_existing_preprocessed_data:\n",
    "#     print('Getting lyrics')\n",
    "#     data = documents #pd.read_csv(working_files_path + '/train.csv')\n",
    "#     data = data.fillna('')  # only the comments has NaN's\n",
    "#     rws = data.lyrics\n",
    "#     print(type(rws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e561e213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:56.829142Z",
     "iopub.status.busy": "2024-11-23T06:07:56.828876Z",
     "iopub.status.idle": "2024-11-23T06:07:56.832403Z",
     "shell.execute_reply": "2024-11-23T06:07:56.831705Z"
    },
    "papermill": {
     "duration": 0.0416,
     "end_time": "2024-11-23T06:07:56.834054",
     "exception": false,
     "start_time": "2024-11-23T06:07:56.792454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sentences, token_lists, idx_in = load_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "613e5066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:56.908944Z",
     "iopub.status.busy": "2024-11-23T06:07:56.908301Z",
     "iopub.status.idle": "2024-11-23T06:07:56.912108Z",
     "shell.execute_reply": "2024-11-23T06:07:56.911286Z"
    },
    "papermill": {
     "duration": 0.042584,
     "end_time": "2024-11-23T06:07:56.913895",
     "exception": false,
     "start_time": "2024-11-23T06:07:56.871311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if load_existing_preprocessed_data:\n",
    "#     sentences, token_lists = load_preprocessed_files(data_file_name)\n",
    "# else:\n",
    "#     sentences, token_lists, idx_in = preprocess(rws)\n",
    "# print(type(sentences), type(token_lists))\n",
    "# # print(np.shape(sentences), np.shape(token_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "122a261e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:07:56.987526Z",
     "iopub.status.busy": "2024-11-23T06:07:56.986925Z",
     "iopub.status.idle": "2024-11-23T06:08:22.433199Z",
     "shell.execute_reply": "2024-11-23T06:08:22.432310Z"
    },
    "papermill": {
     "duration": 25.485564,
     "end_time": "2024-11-23T06:08:22.435311",
     "exception": false,
     "start_time": "2024-11-23T06:07:56.949747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed train checkpoints\n",
      "Reading sentences_input_checkpoint_file\n",
      "Reading token_lists_input_checkpoint_file\n",
      "Reading ngrams_input_checkpoint_file\n",
      "Reading indices_input_checkpoint_file\n",
      "Loaded preprocessed train checkpoints\n",
      "466010 466010 466010\n"
     ]
    }
   ],
   "source": [
    "if load_preprocessed_checkpoints:\n",
    "    print('Loading preprocessed train checkpoints')\n",
    "    train_sentences, train_token_lists, train_idx_in, train_ngram_token_lists = load_progress()\n",
    "    print('Loaded preprocessed train checkpoints')\n",
    "    print(len(train_idx_in), len(train_sentences), len(train_token_lists))\n",
    "else:\n",
    "    print('Enable load_preprocessed_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be7aab3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:08:22.513945Z",
     "iopub.status.busy": "2024-11-23T06:08:22.513597Z",
     "iopub.status.idle": "2024-11-23T06:08:22.518113Z",
     "shell.execute_reply": "2024-11-23T06:08:22.517361Z"
    },
    "papermill": {
     "duration": 0.045205,
     "end_time": "2024-11-23T06:08:22.519759",
     "exception": false,
     "start_time": "2024-11-23T06:08:22.474554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_dim_umap(latent_representations, n_components=30):\n",
    "    # Initialize UMAP model\n",
    "    umap_model = UMAP(n_components=n_components, metric='euclidean', verbose = True)\n",
    "\n",
    "    # Fit UMAP on the entire dataset\n",
    "    final_embeddings = umap_model.fit_transform(latent_representations)\n",
    "\n",
    "    # Save the final embeddings\n",
    "    np.save(\"final_reduced_dim_latent_representations.npy\", final_embeddings)\n",
    "    \n",
    "    return final_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9001d2f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-23T06:08:22.596395Z",
     "iopub.status.busy": "2024-11-23T06:08:22.596117Z"
    },
    "papermill": {
     "duration": 313.417758,
     "end_time": "2024-11-23T06:13:35.976367",
     "exception": false,
     "start_time": "2024-11-23T06:08:22.558609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector file from /kaggle/input/song-recommendation-autoencoder/other/default/2/autoencoder_train_latent_representations_song_lyrics_filtered_seven_hundred_mb.file\n",
      "Loaded vector file from /kaggle/input/song-recommendation-autoencoder-24-dim/other/default/1/autoencoder_train_latent_representations_song_lyrics_filtered_seven_hundred_mb.file\n",
      "[D] [06:08:35.294882] /opt/conda/conda-bld/work/cpp/src/umap/runner.cuh:107 n_neighbors=15\n",
      "[D] [06:08:35.295883] /opt/conda/conda-bld/work/cpp/src/umap/runner.cuh:129 Calling knn graph run\n",
      "[D] [06:10:45.001440] /opt/conda/conda-bld/work/cpp/src/umap/runner.cuh:135 Done. Calling fuzzy simplicial set\n",
      "[D] [06:10:45.018246] /opt/conda/conda-bld/work/cpp/src/umap/fuzzy_simpl_set/naive.cuh:318 Smooth kNN Distances\n",
      "[D] [06:10:45.021180] /opt/conda/conda-bld/work/cpp/src/umap/fuzzy_simpl_set/naive.cuh:320 sigmas = [ 0.143585, 0.452091, 0.217203, 0.358967, 0.25546, 0.162763, 0.289101, 0.403908, 0.17263, 0.68582, 0.285606, 0.330046, 0.355707, 0.35196, 0.221592, 0.183122, 0.405155, 0.190234, 0.281845, 0.299362, 0.251942, 0.518515, 0.145971, 0.177029, 0.23621 ]\n",
      "\n",
      "[D] [06:10:45.021222] /opt/conda/conda-bld/work/cpp/src/umap/fuzzy_simpl_set/naive.cuh:322 rhos = [ 5.93835, 4.82104, 3.64676, 2.99078, 4.72294, 4.94614, 2.79471, 3.39418, 3.95574, 3.18192, 3.29392, 3.87012, 4.0079, 4.08623, 4.24557, 4.75868, 3.63205, 2.8618, 3.97713, 4.14963, 3.57803, 3.33867, 5.50859, 3.31003, 4.77771 ]\n",
      "\n",
      "[D] [06:10:45.021295] /opt/conda/conda-bld/work/cpp/src/umap/fuzzy_simpl_set/naive.cuh:346 Compute Membership Strength\n",
      "[D] [06:11:31.061843] /opt/conda/conda-bld/work/cpp/src/umap/fuzzy_simpl_set/naive.cuh:349 /opt/conda/conda-bld/work/cpp/src/umap/fuzzy_simpl_set/naive.cuh:349 \n",
      "[D] [06:11:31.977186] /opt/conda/conda-bld/work/cpp/src/umap/runner.cuh:142 Done. Calling remove zeros\n",
      "[D] [06:12:22.625094] /opt/conda/conda-bld/work/cpp/src/umap/simpl_set_embed/algo.cuh:347 /opt/conda/conda-bld/work/cpp/src/umap/simpl_set_embed/algo.cuh:347 \n",
      "Loaded vector file from /kaggle/input/song-recommendation-re-ordered-lyrics/other/default/1/reordered_lyrics.pkl\n",
      "Building FAISS index for global candidate retrieval\n"
     ]
    }
   ],
   "source": [
    "if load_existing_model:\n",
    "    tm = load_latest_topic_model(num_topics = ntopic, gamma = gamma)\n",
    "else:\n",
    "    # Define the topic model object\n",
    "    #tm = TopicModel(k = 10), method = TFIDF)\n",
    "    # tm = TopicModel(k = ntopic, gamma = gamma, eps = db_scan_eps, min_samp = db_scan_min_samples, data_file_name = 'data_file_name')\n",
    "    # Fit the topic model by chosen method\n",
    "    # tm.vectorize_tfidf(sentences, max_df = 0.5)\n",
    "    \n",
    "    # tm = TopicModel(\n",
    "    #     data_file_name = data_file_name,\n",
    "    #     token_lists = train_ngram_token_lists,\n",
    "    #     sentences = train_sentences\n",
    "    # )\n",
    "    # tm.get_lda_bert_vectors()\n",
    "    # tm.evaluate_lda_model()\n",
    "    # tm.get_lda_vectors()\n",
    "    # tm.get_bert_vectors()\n",
    "    # tm.tune_lda_hyperparameters()\n",
    "    \n",
    "\n",
    "    # print('Loading preprocessed val checkpoints')\n",
    "    # val_sentences, val_token_lists, val_idx_in, val_ngram_token_lists = load_progress(checkpoints_in_path = val_checkpoints_input_path)\n",
    "    # print(len(val_idx_in), len(val_sentences), len(val_token_lists))\n",
    "    # print('Loaded preprocessed val checkpoints')\n",
    "    # tm_val = TopicModel(\n",
    "    #     data_file_name = data_file_name,\n",
    "    #     token_lists = val_token_lists,\n",
    "    #     sentences = val_sentences,\n",
    "    #     model_name = 'val',\n",
    "    #     tfidf_input_path = val_tfidf_input_path,\n",
    "    #     lda_vec_input_path = val_lda_input_path,\n",
    "    #     bert_input_path = val_song_embeddings_input_path,\n",
    "    # )\n",
    "    # tm_val.get_lda_vectors()\n",
    "    # tm_val.get_bert_vectors()\n",
    "    \n",
    "    # tm.vectorize_tfidf_2(token_lists)\n",
    "    # tm.load_tfidf()\n",
    "    # vec_lda = tm.get_lda_vectors()\n",
    "    # save_vectors('LDA', vec_lda, tm.data_file_name)\n",
    "    # print('LDA vectors saved')\n",
    "\n",
    "    # vec_bert = tm.get_bert_vectors(sentences)\n",
    "    # save_vectors('BERT_preprocessed_sentences', vec_bert, tm.data_file_name)\n",
    "    # print('BERT vectors saved')\n",
    "    # vec_lda = load_vectors_from_path(lda_input_path)\n",
    "    # song_embeddings = load_vectors_from_path(song_embeddings_input_path)\n",
    "\n",
    "\n",
    "    # vec_lda, song_embeddings = tm.get_lda_bert_vectors()\n",
    "    # autoencoder_handler = AutoencoderHandler(vec_lda, song_embeddings, autoencoder_name = 'autoencoder_train', scaled = False, latent_dim=64, num_epochs=20, batch_size=64, checkpoint_dir=autoencoder_checkpoint_dir, log_interval=100, data_file_name=data_file_name)\n",
    "    # autoencoder_handler.train()\n",
    "\n",
    "    # vec_lda, song_embeddings = tm.get_lda_bert_vectors()\n",
    "    # autoencoder_handler = AutoencoderHandler(vec_lda, song_embeddings, autoencoder_name = 'autoencoder_train_24', autoencoder_in_path = autoencoder_input_path_24_dim, scaled = False, latent_dim=24, num_epochs=20, batch_size=64, checkpoint_dir=autoencoder_checkpoint_dir, log_interval=100, data_file_name=data_file_name)\n",
    "    # autoencoder_handler.train()\n",
    "\n",
    "    # vec_lda_val, song_embeddings_val = tm_val.get_lda_bert_vectors()\n",
    "    # autoencoder_handler_val = AutoencoderHandler(vec_lda_val, song_embeddings_val, autoencoder_name = 'autoencoder_val', scaled = False, latent_dim=64, num_epochs=30, batch_size=64, checkpoint_dir=autoencoder_checkpoint_dir, log_interval=100, data_file_name=data_file_name)\n",
    "    # autoencoder_handler_val.generate_validation_representations()\n",
    "\n",
    "    # vec_lda_val, song_embeddings_val = tm_val.get_lda_bert_vectors()\n",
    "    # autoencoder_handler_val = AutoencoderHandler(vec_lda_val, song_embeddings_val, autoencoder_name = 'autoencoder_val_24', autoencoder_in_path = autoencoder_input_path_24_dim, scaled = False, latent_dim=24, num_epochs=30, batch_size=64, checkpoint_dir=autoencoder_checkpoint_dir, log_interval=100, data_file_name=data_file_name)\n",
    "    # autoencoder_handler_val.generate_validation_representations()\n",
    "\n",
    "    latent_representations_file = f'{autoencoder_input_path}/autoencoder_train_latent_representations_song_lyrics_filtered_seven_hundred_mb.file'\n",
    "    latent_representations = load_vectors_from_path(latent_representations_file)\n",
    "    latent_representations_24_dim_file = f'{autoencoder_input_path_24_dim}/autoencoder_train_latent_representations_song_lyrics_filtered_seven_hundred_mb.file'\n",
    "    latent_representations_24_dim = load_vectors_from_path(latent_representations_24_dim_file)\n",
    "    # val_latent_representations_file = f'{val_autoencoder_input_path}/autoencoder_val_latent_representations_song_lyrics_filtered_seven_hundred_mb.file'\n",
    "    # val_latent_representations = load_vectors_from_path(val_latent_representations_file)\n",
    "    # val_latent_representations_24_dim_file = f'{val_autoencoder_input_path_24_dim}/autoencoder_val_latent_representations_song_lyrics_filtered_seven_hundred_mb.file'\n",
    "    # val_latent_representations_24_dim = load_vectors_from_path(val_latent_representations_24_dim_file)\n",
    "    latent_representations_10_dim = reduce_dim_umap(latent_representations_24_dim, 10)\n",
    "\n",
    "    lyrics = load_vectors_from_path(reordered_lyrics_input_path)\n",
    "    recommender = AdvancedSongRecommender(latent_representations, latent_representations_10_dim, lyrics)\n",
    "\n",
    "    # recommendations = recommender.recommend(song_id=10, novelty=0.6)\n",
    "    # logging.info(\"Recommended songs for song_id 10: %s\", recommendations)\n",
    "\n",
    "    # Assume unseen_song_rep is the latent representation of a new song\n",
    "    # unseen_song_rep = np.random.rand(64)\n",
    "    # recommendations, theme_explanations = recommender.recommend_for_unseen_song(unseen_song_rep, novelty=0.6)\n",
    "    # logging.info(\"Recommended songs for unseen song: %s\", recommendations)\n",
    "    # logging.info(\"Theme explanations: %s\", theme_explanations)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc98c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T18:44:05.364787Z",
     "iopub.status.busy": "2024-11-17T18:44:05.364406Z",
     "iopub.status.idle": "2024-11-17T18:44:05.371160Z",
     "shell.execute_reply": "2024-11-17T18:44:05.370049Z",
     "shell.execute_reply.started": "2024-11-17T18:44:05.364748Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5930008,
     "sourceId": 9698123,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 492658,
     "sourceId": 2378330,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 159918,
     "modelInstanceId": 137200,
     "sourceId": 161358,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 156782,
     "modelInstanceId": 134015,
     "sourceId": 162369,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 148264,
     "modelInstanceId": 125280,
     "sourceId": 162601,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 158364,
     "modelInstanceId": 135633,
     "sourceId": 167308,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 160548,
     "modelInstanceId": 137858,
     "sourceId": 169922,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 160861,
     "modelInstanceId": 138191,
     "sourceId": 169924,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 160862,
     "modelInstanceId": 138192,
     "sourceId": 169925,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 167197,
     "modelInstanceId": 144637,
     "sourceId": 170008,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 158360,
     "modelInstanceId": 135629,
     "sourceId": 170085,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 159715,
     "modelInstanceId": 136996,
     "sourceId": 172644,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 170253,
     "modelInstanceId": 147722,
     "sourceId": 173542,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171002,
     "modelInstanceId": 148484,
     "sourceId": 174406,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 171087,
     "modelInstanceId": 148562,
     "sourceId": 174490,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 159987,
     "modelInstanceId": 137274,
     "sourceId": 174506,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 553.115222,
   "end_time": "2024-11-23T06:13:36.015062",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-23T06:04:22.899840",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
